{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lost & Found for colby\n",
        "- recognize everyday objects students commonly lose â€” like phones, wallets, AirPods, glasses, cards, and more â€‹\n",
        "\n",
        "- to automatically categorize them into a searchable website interface.â€‹\n",
        "\n",
        "- So that you donâ€™t have to walk in snow to check whether your lost items is in the lost and found center!â€‹"
      ],
      "metadata": {
        "id": "xaCjTXt0xeGJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOn8iIREioWp",
        "outputId": "fba090ad-b9c4-40d3-987a-a61b17db82fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.234-py3-none-any.whl.metadata (37 kB)\n",
            "Collecting roboflow\n",
            "  Downloading roboflow-1.2.11-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.24.0+cu126)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.31.0)\n",
            "Collecting ultralytics-thop>=2.0.18 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from roboflow) (2025.11.12)\n",
            "Collecting idna==3.7 (from roboflow)\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.12/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.4.9)\n",
            "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n",
            "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting pi-heif<2 (from roboflow)\n",
            "  Downloading pi_heif-1.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.5 kB)\n",
            "Collecting pillow-avif-plugin<2 (from roboflow)\n",
            "  Downloading pillow_avif_plugin-1.5.2-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.9.0.post0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.5.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from roboflow) (4.67.1)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.0.0)\n",
            "Collecting filetype (from roboflow)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.60.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n",
            "Downloading ultralytics-8.3.234-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading roboflow-1.2.11-py3-none-any.whl (89 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pi_heif-1.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow_avif_plugin-1.5.2-cp312-cp312-manylinux_2_28_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m107.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Installing collected packages: pillow-avif-plugin, filetype, pi-heif, opencv-python-headless, idna, ultralytics-thop, roboflow, ultralytics\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.12.0.88\n",
            "    Uninstalling opencv-python-headless-4.12.0.88:\n",
            "      Successfully uninstalled opencv-python-headless-4.12.0.88\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.11\n",
            "    Uninstalling idna-3.11:\n",
            "      Successfully uninstalled idna-3.11\n",
            "Successfully installed filetype-1.2.0 idna-3.7 opencv-python-headless-4.10.0.84 pi-heif-1.1.1 pillow-avif-plugin-1.5.2 roboflow-1.2.11 ultralytics-8.3.234 ultralytics-thop-2.0.18\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics roboflow opencv-python pillow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive; drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LQaQ_wNjmk5",
        "outputId": "86a3e468-b194-4fcb-a288-93a59d4c83a4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "\n",
        "project_dir = \"/content/drive/MyDrive/Colab Notebooks/lost_found_project\"\n",
        "os.chdir(project_dir)\n",
        "sys.path.append(project_dir)\n",
        "\n",
        "!ls -lh\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AzGVK0V5nXJ",
        "outputId": "fc9e4020-361c-4869-d5a4-7084cb2a5581"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 42K\n",
            "-rw------- 1 root root  14K Dec  1 08:44 'Computer vision final project - Ariel.ipynb'\n",
            "drwx------ 2 root root 4.0K Dec  1 08:37  converted_images\n",
            "-rw------- 1 root root 8.2K Dec  1 08:19  dataset_preparation.py\n",
            "drwx------ 2 root root 4.0K Dec  1 08:01  __pycache__\n",
            "drwx------ 2 root root 4.0K Dec  1 06:21  raw_images\n",
            "drwx------ 2 root root 4.0K Dec  1 08:01  resized_images\n",
            "drwx------ 2 root root 4.0K Dec  1 08:34  validated_images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install roboflow ultralytics pillow pyyaml -q"
      ],
      "metadata": {
        "id": "c7TDOA152HuW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataset_preparation import resize_images, merge_yolo_datasets, verify_dataset, visualize_samples\n",
        "\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import yaml\n",
        "from PIL import Image, ImageDraw, ImageFile\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "\n",
        "# Allow loading truncated images\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True"
      ],
      "metadata": {
        "id": "1AkAK1kS33iE"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import dataset_preparation\n",
        "importlib.reload(dataset_preparation)\n",
        "\n",
        "from dataset_preparation import resize_images, merge_yolo_datasets, verify_dataset, visualize_samples"
      ],
      "metadata": {
        "id": "1uYZMs8LEjt5"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Allow loading truncated images\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "# Install pillow-heif for HEIC support\n",
        "print(\"Installing pillow-heif...\")\n",
        "os.system('pip install pillow-heif -q')\n",
        "\n",
        "from pillow_heif import register_heif_opener\n",
        "register_heif_opener()\n",
        "\n",
        "# Define paths\n",
        "raw_images_path = \"/content/drive/MyDrive/Colab Notebooks/lost_found_project/raw_images\"\n",
        "converted_path = \"/content/drive/MyDrive/Colab Notebooks/lost_found_project/converted_images\"\n",
        "resized_output_path = \"/content/drive/MyDrive/Colab Notebooks/lost_found_project/resized_images\"\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(converted_path, exist_ok=True)\n",
        "\n",
        "# Convert HEIC to JPG\n",
        "print(\"\\n Converting HEIC to JPG\")\n",
        "converted = 0\n",
        "copied = 0\n",
        "failed = []\n",
        "\n",
        "for img_name in os.listdir(raw_images_path):\n",
        "    if not img_name.lower().endswith(('.jpg', '.jpeg', '.png', '.heic', '.heif')):\n",
        "        continue\n",
        "\n",
        "    img_path = os.path.join(raw_images_path, img_name)\n",
        "    base_name = os.path.splitext(img_name)[0]\n",
        "    output_name = f\"{base_name}.jpg\"\n",
        "    output_path = os.path.join(converted_path, output_name)\n",
        "\n",
        "    try:\n",
        "        img = Image.open(img_path)\n",
        "\n",
        "        if img.mode != 'RGB':\n",
        "            img = img.convert('RGB')\n",
        "\n",
        "        img.save(output_path, 'JPEG', quality=95)\n",
        "\n",
        "        if img_name.lower().endswith(('.heic', '.heif')):\n",
        "            converted += 1\n",
        "            print(f\"Converted: {img_name}\")\n",
        "        else:\n",
        "            copied += 1\n",
        "\n",
        "    except Exception as e:\n",
        "        failed.append((img_name, str(e)))\n",
        "        print(f\"Failed: {img_name}\")\n",
        "\n",
        "print(f\"\\nConverted: {converted}, Copied: {copied}, Failed: {len(failed)}\")\n",
        "\n",
        "# Resize images using function from dataset_preparation\n",
        "print(\"\\nResizing images to (640,640)\")\n",
        "num_processed = resize_images(converted_path, resized_output_path, target_size=(640, 640))\n",
        "\n",
        "print(f\"\\nTotal processed: {num_processed}\")\n",
        "\n",
        "# Create ZIP for Roboflow\n",
        "print(\"\\nCreating ZIP file\")\n",
        "zip_path = '/content/resized_images_for_roboflow'\n",
        "shutil.make_archive(zip_path, 'zip', resized_output_path)\n",
        "files.download(f'{zip_path}.zip')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tjjxsRorFoo4",
        "outputId": "eb5c369f-ff61-4685-b6d8-77f7d5d142a9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing pillow-heif...\n",
            "\n",
            " Converting HEIC to JPG\n",
            "\n",
            "Converted: 0, Copied: 44, Failed: 0\n",
            "\n",
            "Resizing images to (640,640)\n",
            "Processed: new place ðŸ”‘ #ÑÐ°Ð¼Ð¾Ñ€Ð°Ð·Ð²Ð¸Ñ‚Ð¸Ðµ #Ð¿ÑÐ¸Ñ…Ð¾Ð»Ð¾Ð³Ð¸Ñâ€¦.jpg -> img_0000.jpg\n",
            "Processed: Keys.jpg -> img_0001.jpg\n",
            "Processed: truth.jpg -> img_0002.jpg\n",
            "Processed: New house keys _ Premium Photo.jpg -> img_0003.jpg\n",
            "Processed: The Key You Canâ€™t Ask For.jpg -> img_0004.jpg\n",
            "Processed: House key on plans stock photo_ Image of plans, extension - 21591266.jpg -> img_0005.jpg\n",
            "Processed: Vintage _Random Lockers_ Key Set - (#18 Total).jpg -> img_0006.jpg\n",
            "Processed: KEYS_2929.jpg -> img_0007.jpg\n",
            "Processed: KEYS_2928.jpg -> img_0008.jpg\n",
            "Processed: KEYS_2927.jpg -> img_0009.jpg\n",
            "Processed: KEYS_2926.jpg -> img_0010.jpg\n",
            "Processed: KEYS_2925.jpg -> img_0011.jpg\n",
            "Processed: KEYS_2924.jpg -> img_0012.jpg\n",
            "Processed: KEYS_2923.jpg -> img_0013.jpg\n",
            "Processed: KEYS_2922.jpg -> img_0014.jpg\n",
            "Processed: keys (1).jpg -> img_0015.jpg\n",
            "Processed: IMG_2957.jpg -> img_0016.jpg\n",
            "Processed: IMG_2956.jpg -> img_0017.jpg\n",
            "Processed: IMG_2955.jpg -> img_0018.jpg\n",
            "Processed: IMG_2954.jpg -> img_0019.jpg\n",
            "Processed: IMG_2953.jpg -> img_0020.jpg\n",
            "Processed: IMG_2952.jpg -> img_0021.jpg\n",
            "Processed: IMG_2951.jpg -> img_0022.jpg\n",
            "Processed: IMG_2950.jpg -> img_0023.jpg\n",
            "Processed: IMG_2949.jpg -> img_0024.jpg\n",
            "Processed: IMG_2948.jpg -> img_0025.jpg\n",
            "Processed: IMG_2947.jpg -> img_0026.jpg\n",
            "Processed: IMG_2946.jpg -> img_0027.jpg\n",
            "Processed: IMG_2945.jpg -> img_0028.jpg\n",
            "Processed: colbycard_2944.jpg -> img_0029.jpg\n",
            "Processed: colbycard_2943.jpg -> img_0030.jpg\n",
            "Processed: colbycard_2942.jpg -> img_0031.jpg\n",
            "Processed: colbycard_2941.jpg -> img_0032.jpg\n",
            "Processed: colbycard_2940.jpg -> img_0033.jpg\n",
            "Processed: colbycard_2939.jpg -> img_0034.jpg\n",
            "Processed: colbycard_2938.jpg -> img_0035.jpg\n",
            "Processed: colbycard_2937.jpg -> img_0036.jpg\n",
            "Processed: colbycard_2936.jpg -> img_0037.jpg\n",
            "Processed: colbycard_2935.jpg -> img_0038.jpg\n",
            "Processed: colbycard_2934.jpg -> img_0039.jpg\n",
            "Processed: colbycard_2933.jpg -> img_0040.jpg\n",
            "Processed: colbycard_2932.jpg -> img_0041.jpg\n",
            "Processed: colbycard_2931.jpg -> img_0042.jpg\n",
            "Processed: colbycard_2930.jpg -> img_0043.jpg\n",
            "\n",
            "Total processed: 44/44\n",
            "\n",
            "Total processed: 44\n",
            "\n",
            "Creating ZIP file\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e21b1c8f-ddc7-467f-91ab-f8f5248aa229\", \"resized_images_for_roboflow.zip\", 6373447)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from dataset_preparation import merge_yolo_datasets, verify_dataset, visualize_samples\n",
        "\n",
        "# Install roboflow\n",
        "os.system('pip install roboflow -q')\n",
        "\n",
        "from roboflow import Roboflow"
      ],
      "metadata": {
        "id": "PuQsSY5sNwXD"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Downloading AirPods dataset...\")\n",
        "rf = Roboflow(api_key=\"tSpyQRRhpZElHqZ7Y81V\")\n",
        "project = rf.workspace(\"project-uzwxb\").project(\"airpods-7mme5\")\n",
        "version = project.version(7)\n",
        "dataset1 = version.download(\"yolov8\")\n",
        "dataset1_path = dataset1.location\n",
        "print(f\"Dataset 1 downloaded to: {dataset1_path}\")\n",
        "\n",
        "# Download Dataset 2: Battery/Charger/Cable\n",
        "print(\"\\nDownloading Battery/Charger/Cable dataset...\")\n",
        "rf = Roboflow(api_key=\"tSpyQRRhpZElHqZ7Y81V\")\n",
        "project = rf.workspace(\"boomaie\").project(\"battery-charger-cable-82kib\")\n",
        "version = project.version(2)\n",
        "dataset2 = version.download(\"yolov8\")\n",
        "dataset2_path = dataset2.location\n",
        "print(f\"Dataset 2 downloaded to: {dataset2_path}\")\n",
        "\n",
        "# Download Dataset 3: My custom annotated dataset (mainly Colby cards)\n",
        "print(\"\\nDownloading my custom dataset...\")\n",
        "rf = Roboflow(api_key=\"tSpyQRRhpZElHqZ7Y81V\")\n",
        "project = rf.workspace(\"yilin-pan-gzs5y\").project(\"lostandfind-06kgb\")\n",
        "version = project.version(1)\n",
        "dataset3 = version.download(\"yolov8\")\n",
        "dataset3_path = dataset3.location\n",
        "print(f\"Dataset 3 downloaded to: {dataset3_path}\")\n",
        "\n",
        "# Download Dataset 4: Scarfs\n",
        "print(\"\\nDownloading Scarfs dataset...\")\n",
        "rf = Roboflow(api_key=\"tSpyQRRhpZElHqZ7Y81V\")\n",
        "project = rf.workspace(\"practicas-6jgbv\").project(\"scarf-zkmzt\")\n",
        "version = project.version(1)\n",
        "dataset4 = version.download(\"yolov8\")\n",
        "dataset4_path = dataset4.location\n",
        "print(f\"Dataset 4 downloaded to: {dataset4_path}\")\n",
        "\n",
        "# Download Dataset 5: Jackets/Vests\n",
        "print(\"\\nDownloading Jackets/Vests dataset...\")\n",
        "rf = Roboflow(api_key=\"tSpyQRRhpZElHqZ7Y81V\")\n",
        "project = rf.workspace(\"meister1\").project(\"jackets-vests\")\n",
        "version = project.version(1)\n",
        "dataset5 = version.download(\"yolov8\")\n",
        "dataset5_path = dataset5.location\n",
        "print(f\"Dataset 5 downloaded to: {dataset5_path}\")\n",
        "\n",
        "# Doanload Dataset 6: General Lost and Found\n",
        "print(\"\\nDownloading general lost and fonud dataset...\")\n",
        "rf = Roboflow(api_key=\"tSpyQRRhpZElHqZ7Y81V\")\n",
        "project = rf.workspace(\"mercys-workspace-7ti14\").project(\"lost-and-found-items\")\n",
        "version = project.version(7)\n",
        "dataset6 = version.download(\"yolov8\")\n",
        "dataset6_path = dataset6.location\n",
        "print(f\"Dataset 6 downloaded to: {dataset6_path}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CiA7313N0GX",
        "outputId": "4c283a8d-c9cc-44a9-d1cd-da9f93736393"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading AirPods dataset...\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Dataset 1 downloaded to: /content/drive/MyDrive/Colab Notebooks/lost_found_project/Airpods-7\n",
            "\n",
            "Downloading Battery/Charger/Cable dataset...\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Dataset 2 downloaded to: /content/drive/MyDrive/Colab Notebooks/lost_found_project/Battery-charger-cable-2\n",
            "\n",
            "Downloading my custom dataset...\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Dataset 3 downloaded to: /content/drive/MyDrive/Colab Notebooks/lost_found_project/LostandFind-1\n",
            "\n",
            "Downloading Scarfs dataset...\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Dataset 4 downloaded to: /content/drive/MyDrive/Colab Notebooks/lost_found_project/Scarf-1\n",
            "\n",
            "Downloading Jackets/Vests dataset...\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Dataset 5 downloaded to: /content/drive/MyDrive/Colab Notebooks/lost_found_project/jackets-vests-1\n",
            "\n",
            "Downloading general lost and fonud dataset...\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in Lost-and-found-items-7 to yolov8:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103745/103745 [00:09<00:00, 11459.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to Lost-and-found-items-7 in yolov8:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4358/4358 [00:37<00:00, 116.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset 6 downloaded to: /content/drive/MyDrive/Colab Notebooks/lost_found_project/Lost-and-found-items-7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "import os\n",
        "\n",
        "def verify_dataset_fixed(dataset_path):\n",
        "    \"\"\"Verify dataset structure and print statistics\"\"\"\n",
        "    print(f\"\\nVerifying: {dataset_path}\\n\")\n",
        "\n",
        "    stats = {\n",
        "        'train_images': 0,\n",
        "        'train_labels': 0,\n",
        "        'valid_images': 0,\n",
        "        'valid_labels': 0,\n",
        "        'test_images': 0,\n",
        "        'test_labels': 0,\n",
        "        'classes': {}\n",
        "    }\n",
        "\n",
        "    for split in ['train', 'valid', 'test']:\n",
        "        img_dir = f'{dataset_path}/{split}/images'\n",
        "        lbl_dir = f'{dataset_path}/{split}/labels'\n",
        "\n",
        "        if os.path.exists(img_dir):\n",
        "            stats[f'{split}_images'] = len(os.listdir(img_dir))\n",
        "        if os.path.exists(lbl_dir):\n",
        "            stats[f'{split}_labels'] = len(os.listdir(lbl_dir))\n",
        "\n",
        "    yaml_path = f'{dataset_path}/data.yaml'\n",
        "    if os.path.exists(yaml_path):\n",
        "        with open(yaml_path, 'r') as f:\n",
        "            data = yaml.safe_load(f)\n",
        "        stats['classes'] = data['names']\n",
        "\n",
        "    # Handle both dict and list formats\n",
        "    if isinstance(stats['classes'], dict):\n",
        "        class_list = list(stats['classes'].values())\n",
        "    else:\n",
        "        class_list = stats['classes']\n",
        "\n",
        "    print(\"Dataset Summary:\")\n",
        "    print(f\"  Train: {stats['train_images']} images, {stats['train_labels']} labels\")\n",
        "    print(f\"  Valid: {stats['valid_images']} images, {stats['valid_labels']} labels\")\n",
        "    print(f\"  Test: {stats['test_images']} images, {stats['test_labels']} labels\")\n",
        "    print(f\"  Classes: {class_list}\")\n",
        "\n",
        "    if stats['train_images'] != stats['train_labels']:\n",
        "        print(f\"\\nWarning: Train images/labels mismatch\")\n",
        "    if stats['valid_images'] != stats['valid_labels']:\n",
        "        print(f\"Warning: Valid images/labels mismatch\")\n",
        "\n",
        "    return stats\n"
      ],
      "metadata": {
        "id": "jj_QcYOsQ5HB"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_yolo_datasets_with_remap(dataset_paths, output_path, dataset_prefixes=None):\n",
        "\n",
        "    if dataset_prefixes is None:\n",
        "        dataset_prefixes = [f\"ds{i}\" for i in range(len(dataset_paths))]\n",
        "\n",
        "    # Create output structure\n",
        "    for split in ['train', 'valid', 'test']:\n",
        "        os.makedirs(f'{output_path}/{split}/images', exist_ok=True)\n",
        "        os.makedirs(f'{output_path}/{split}/labels', exist_ok=True)\n",
        "\n",
        "    # Collect all unique classes across datasets\n",
        "    all_class_names = []\n",
        "    class_mappings = []\n",
        "\n",
        "    for dataset_path in dataset_paths:\n",
        "        yaml_path = f'{dataset_path}/data.yaml'\n",
        "        with open(yaml_path, 'r') as f:\n",
        "            data = yaml.safe_load(f)\n",
        "\n",
        "        dataset_classes = data['names']\n",
        "        if isinstance(dataset_classes, dict):\n",
        "            dataset_classes = list(dataset_classes.values())\n",
        "\n",
        "        print(f\"  {dataset_path}: {dataset_classes}\")\n",
        "\n",
        "        # Create mapping for this dataset\n",
        "        old_to_new = {}\n",
        "        for old_id, class_name in enumerate(dataset_classes):\n",
        "            if class_name not in all_class_names:\n",
        "                all_class_names.append(class_name)\n",
        "\n",
        "            # Map old ID to new ID\n",
        "            new_id = all_class_names.index(class_name)\n",
        "            old_to_new[old_id] = new_id\n",
        "\n",
        "        class_mappings.append(old_to_new)\n",
        "\n",
        "    # Convert class names list to dict\n",
        "    all_classes = {i: name for i, name in enumerate(all_class_names)}\n",
        "    print(f\"\\nMerged classes: {all_classes}\")\n",
        "\n",
        "    # Process each dataset\n",
        "    for dataset_idx, (dataset_path, prefix) in enumerate(zip(dataset_paths, dataset_prefixes)):\n",
        "        print(f\"\\nProcessing dataset: {dataset_path}\")\n",
        "        class_mapping = class_mappings[dataset_idx]\n",
        "\n",
        "        for split in ['train', 'valid', 'test']:\n",
        "            img_dir = f'{dataset_path}/{split}/images'\n",
        "            lbl_dir = f'{dataset_path}/{split}/labels'\n",
        "\n",
        "            if not os.path.exists(img_dir):\n",
        "                continue\n",
        "\n",
        "            # Copy images\n",
        "            img_count = 0\n",
        "            for img in os.listdir(img_dir):\n",
        "                new_name = f\"{prefix}_{img}\"\n",
        "                shutil.copy(\n",
        "                    os.path.join(img_dir, img),\n",
        "                    f'{output_path}/{split}/images/{new_name}'\n",
        "                )\n",
        "                img_count += 1\n",
        "\n",
        "            # Copy and remap labels\n",
        "            lbl_count = 0\n",
        "            if os.path.exists(lbl_dir):\n",
        "                for lbl in os.listdir(lbl_dir):\n",
        "                    lbl_path = os.path.join(lbl_dir, lbl)\n",
        "\n",
        "                    # Read and remap labels\n",
        "                    with open(lbl_path, 'r') as f:\n",
        "                        lines = f.readlines()\n",
        "\n",
        "                    remapped_lines = []\n",
        "                    for line in lines:\n",
        "                        parts = line.strip().split()\n",
        "                        if len(parts) < 5:\n",
        "                            continue\n",
        "\n",
        "                        old_class_id = int(parts[0])\n",
        "                        new_class_id = class_mapping.get(old_class_id, old_class_id)\n",
        "                        parts[0] = str(new_class_id)\n",
        "                        remapped_lines.append(' '.join(parts) + '\\n')\n",
        "\n",
        "                    # Write remapped labels\n",
        "                    new_name = f\"{prefix}_{lbl}\"\n",
        "                    with open(f'{output_path}/{split}/labels/{new_name}', 'w') as f:\n",
        "                        f.writelines(remapped_lines)\n",
        "                    lbl_count += 1\n",
        "\n",
        "            print(f\"  {split}: {img_count} images, {lbl_count} labels\")\n",
        "\n",
        "    # Create merged data.yaml\n",
        "    merged_yaml = {\n",
        "        'path': output_path,\n",
        "        'train': 'train/images',\n",
        "        'val': 'valid/images',\n",
        "        'test': 'test/images',\n",
        "        'names': all_classes\n",
        "    }\n",
        "\n",
        "    with open(f'{output_path}/data.yaml', 'w') as f:\n",
        "        yaml.dump(merged_yaml, f)\n",
        "\n",
        "    print(f\"\\nMerged dataset created at: {output_path}\")\n",
        "    print(f\"Total classes: {len(all_classes)}\")\n",
        "    print(f\"Classes: {list(all_classes.values())}\")\n",
        "\n",
        "    return output_path"
      ],
      "metadata": {
        "id": "2zBWQSclR8SS"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "# Dataset paths\n",
        "dataset_paths = [\n",
        "    dataset1_path,  # airpods\n",
        "    dataset2_path,  # battery-charger-cable\n",
        "    dataset3_path,  # custom lost&found\n",
        "    dataset4_path,  # scarf\n",
        "    dataset5_path,   # jackets-vests\n",
        "    dataset6_path  # general lost-and-found\n",
        "  ]\n",
        "\n",
        "dataset_prefixes = ['airpods', 'battery', 'custom', 'scarf', 'jackets','lostandfound']\n",
        "\n",
        "# Where to save the merged dataset\n",
        "output_path = '/content/drive/MyDrive/Colab Notebooks/lost_found_project/final_dataset'\n",
        "\n",
        "# Classes to exclude (there's some issue with my own airpods annotation, we will be using downloaded airpods images instead)\n",
        "exclude_classes = ['airpods', 'airpods-whole', 'none']\n",
        "\n",
        "# Clean output\n",
        "if os.path.exists(output_path):\n",
        "    shutil.rmtree(output_path)\n",
        "\n",
        "for split in ['train', 'valid', 'test']:\n",
        "    os.makedirs(f'{output_path}/{split}/images', exist_ok=True)\n",
        "    os.makedirs(f'{output_path}/{split}/labels', exist_ok=True)\n",
        "\n",
        "\n",
        "all_class_names = []\n",
        "\n",
        "print(\"\\nCollecting classes:\\n\")\n",
        "for dataset_path in dataset_paths:\n",
        "    yaml_path = os.path.join(dataset_path, 'data.yaml')\n",
        "    if not os.path.exists(yaml_path):\n",
        "        print(f\"  Warning: {yaml_path} not found, skipping.\")\n",
        "        continue\n",
        "\n",
        "    with open(yaml_path, 'r') as f:\n",
        "        data = yaml.safe_load(f)\n",
        "\n",
        "    classes = data['names']\n",
        "    if isinstance(classes, dict):\n",
        "        classes = list(classes.values())\n",
        "\n",
        "    print(f\"{os.path.basename(dataset_path)}: {classes}\")\n",
        "\n",
        "    for class_name in classes:\n",
        "        if class_name not in exclude_classes and class_name not in all_class_names:\n",
        "            all_class_names.append(class_name)\n",
        "\n",
        "print(f\"\\nFinal classes (after filtering): {all_class_names}\\n\")\n",
        "\n",
        "# Create ID mappings from each dataset to the unified class list\n",
        "\n",
        "class_mappings = []\n",
        "for dataset_path in dataset_paths:\n",
        "    yaml_path = os.path.join(dataset_path, 'data.yaml')\n",
        "    if not os.path.exists(yaml_path):\n",
        "        class_mappings.append({})\n",
        "        continue\n",
        "\n",
        "    with open(yaml_path, 'r') as f:\n",
        "        data = yaml.safe_load(f)\n",
        "\n",
        "    classes = data['names']\n",
        "    if isinstance(classes, dict):\n",
        "        classes = list(classes.values())\n",
        "\n",
        "    mapping = {}\n",
        "    for old_id, class_name in enumerate(classes):\n",
        "        if class_name in all_class_names:\n",
        "            mapping[old_id] = all_class_names.index(class_name)\n",
        "\n",
        "    class_mappings.append(mapping)\n",
        "\n",
        "# Copy and filter files for each dataset and split\n",
        "\n",
        "for dataset_idx, (dataset_path, prefix) in enumerate(zip(dataset_paths, dataset_prefixes)):\n",
        "    print(f\"Processing {prefix} from {dataset_path} ...\")\n",
        "    mapping = class_mappings[dataset_idx]\n",
        "\n",
        "    for split in ['train', 'valid', 'test']:\n",
        "        img_dir = os.path.join(dataset_path, split, 'images')\n",
        "        lbl_dir = os.path.join(dataset_path, split, 'labels')\n",
        "\n",
        "        if not os.path.exists(img_dir):\n",
        "            continue\n",
        "\n",
        "        for img_name in os.listdir(img_dir):\n",
        "            if not img_name.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.webp')):\n",
        "                continue\n",
        "\n",
        "            lbl_name = os.path.splitext(img_name)[0] + '.txt'\n",
        "            lbl_path = os.path.join(lbl_dir, lbl_name)\n",
        "\n",
        "            if not os.path.exists(lbl_path):\n",
        "                continue\n",
        "\n",
        "            # Read and filter labels\n",
        "            with open(lbl_path, 'r') as f:\n",
        "                lines = f.readlines()\n",
        "\n",
        "            remapped = []\n",
        "            for line in lines:\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) >= 5:\n",
        "                    old_id = int(parts[0])\n",
        "                    if old_id in mapping:\n",
        "                        parts[0] = str(mapping[old_id])\n",
        "                        remapped.append(' '.join(parts) + '\\n')\n",
        "\n",
        "            # Only copy if we have valid labels after filtering\n",
        "            if remapped:\n",
        "                shutil.copy(\n",
        "                    os.path.join(img_dir, img_name),\n",
        "                    os.path.join(output_path, split, 'images', f'{prefix}_{img_name}')\n",
        "                )\n",
        "\n",
        "                with open(os.path.join(output_path, split, 'labels', f'{prefix}_{lbl_name}'), 'w') as f:\n",
        "                    f.writelines(remapped)\n",
        "\n",
        "# Create validation split from training data\n",
        "\n",
        "print(\"\\nCreating validation split from training data...\")\n",
        "train_img_dir = os.path.join(output_path, 'train', 'images')\n",
        "train_lbl_dir = os.path.join(output_path, 'train', 'labels')\n",
        "\n",
        "all_train_images = os.listdir(train_img_dir)\n",
        "num_train = len(all_train_images)\n",
        "num_valid = int(num_train * 0.15)\n",
        "\n",
        "random.shuffle(all_train_images)\n",
        "valid_images = all_train_images[:num_valid]\n",
        "\n",
        "for img_name in valid_images:\n",
        "    lbl_name = os.path.splitext(img_name)[0] + '.txt'\n",
        "\n",
        "    shutil.move(\n",
        "        os.path.join(train_img_dir, img_name),\n",
        "        os.path.join(output_path, 'valid', 'images', img_name)\n",
        "    )\n",
        "\n",
        "    lbl_src = os.path.join(train_lbl_dir, lbl_name)\n",
        "    if os.path.exists(lbl_src):\n",
        "        shutil.move(\n",
        "            lbl_src,\n",
        "            os.path.join(output_path, 'valid', 'labels', lbl_name)\n",
        "        )\n",
        "\n",
        "# Create final data.yaml\n",
        "final_classes = {i: name for i, name in enumerate(all_class_names)}\n",
        "\n",
        "yaml_content = {\n",
        "    'path': output_path,\n",
        "    'train': 'train/images',\n",
        "    'val': 'valid/images',\n",
        "    'test': 'test/images',\n",
        "    'names': final_classes\n",
        "}\n",
        "\n",
        "with open(os.path.join(output_path, 'data.yaml'), 'w') as f:\n",
        "    yaml.dump(yaml_content, f)\n",
        "\n",
        "# Verify\n",
        "print(f\"\\nDataset created at: {output_path}\")\n",
        "print(f\"Classes ({len(final_classes)}): {list(final_classes.values())}\")\n",
        "\n",
        "train_count = len(os.listdir(os.path.join(output_path, 'train', 'images')))\n",
        "valid_count = len(os.listdir(os.path.join(output_path, 'valid', 'images')))\n",
        "total = train_count + valid_count\n",
        "\n",
        "print(f\"\\nTrain: {train_count} ({train_count / total * 100:.1f}%)\")\n",
        "print(f\"Valid: {valid_count} ({valid_count / total * 100:.1f}%)\")\n",
        "print(f\"Total: {total}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Coug0g3UQV5Y",
        "outputId": "8a336549-d830-472d-f766-95fbae6e532f"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting classes:\n",
            "\n",
            "Airpods-7: ['Airpods']\n",
            "Battery-charger-cable-2: ['c to c', 'c toc', 'lightning to c', 'usb in 3', 'usb to  LIghtning', 'usb to c', 'usb to micro-usb']\n",
            "LostandFind-1: ['airpods', 'airpods-whole', 'colby student card', 'keys', 'none', 'powerbank']\n",
            "Scarf-1: ['Scarf']\n",
            "jackets-vests-1: ['jackets', 'vests']\n",
            "Lost-and-found-items-7: ['Backpack', 'Belt-bag', 'Car-key-Acura', 'Car-key-Chevrolet', 'Car-key-Ford', 'Car-key-Honda', 'Car-key-Hyundai', 'Car-key-Kia', 'Car-key-Lexus', 'Car-key-Mercedes-Benz', 'Car-key-Mitsubishi', 'Car-key-Nissan', 'Car-key-Toyota', 'Car-key-Volkswagen', 'Car-key-non-specific', 'Card', 'Cross-bag', 'Electronic-devices-AirPods', 'Electronic-devices-AirPods-case', 'Electronic-devices-charger', 'Electronic-devices-earphones', 'Electronic-devices-headphones', 'Electronic-devices-laptop', 'Electronic-devices-laptop-charger', 'Electronic-devices-phone', 'Electronic-devices-phone-charger', 'Electronic-devices-powerbank', 'Electronic-devices-tablet', 'Glasses', 'Handbag', 'Key', 'Laptop-bag', 'Smart-watch-black', 'Smart-watch-gold', 'Smart-watch-pink', 'Smart-watch-silver', 'Smart-watch-white', 'Sunglasses', 'Travel-bag', 'Wallet-black', 'Wallet-brown', 'Wallet-grey', 'Wallet-multicolor', 'Wallet-pink', 'Wallet-red', 'Wallet-turquoise', 'Watch-black', 'Watch-blue', 'Watch-brown', 'Watch-gold', 'Watch-silver', 'Watch-white']\n",
            "\n",
            "Final classes (after filtering): ['Airpods', 'c to c', 'c toc', 'lightning to c', 'usb in 3', 'usb to  LIghtning', 'usb to c', 'usb to micro-usb', 'colby student card', 'keys', 'powerbank', 'Scarf', 'jackets', 'vests', 'Backpack', 'Belt-bag', 'Car-key-Acura', 'Car-key-Chevrolet', 'Car-key-Ford', 'Car-key-Honda', 'Car-key-Hyundai', 'Car-key-Kia', 'Car-key-Lexus', 'Car-key-Mercedes-Benz', 'Car-key-Mitsubishi', 'Car-key-Nissan', 'Car-key-Toyota', 'Car-key-Volkswagen', 'Car-key-non-specific', 'Card', 'Cross-bag', 'Electronic-devices-AirPods', 'Electronic-devices-AirPods-case', 'Electronic-devices-charger', 'Electronic-devices-earphones', 'Electronic-devices-headphones', 'Electronic-devices-laptop', 'Electronic-devices-laptop-charger', 'Electronic-devices-phone', 'Electronic-devices-phone-charger', 'Electronic-devices-powerbank', 'Electronic-devices-tablet', 'Glasses', 'Handbag', 'Key', 'Laptop-bag', 'Smart-watch-black', 'Smart-watch-gold', 'Smart-watch-pink', 'Smart-watch-silver', 'Smart-watch-white', 'Sunglasses', 'Travel-bag', 'Wallet-black', 'Wallet-brown', 'Wallet-grey', 'Wallet-multicolor', 'Wallet-pink', 'Wallet-red', 'Wallet-turquoise', 'Watch-black', 'Watch-blue', 'Watch-brown', 'Watch-gold', 'Watch-silver', 'Watch-white']\n",
            "\n",
            "Processing airpods from /content/drive/MyDrive/Colab Notebooks/lost_found_project/Airpods-7 ...\n",
            "Processing battery from /content/drive/MyDrive/Colab Notebooks/lost_found_project/Battery-charger-cable-2 ...\n",
            "Processing custom from /content/drive/MyDrive/Colab Notebooks/lost_found_project/LostandFind-1 ...\n",
            "Processing scarf from /content/drive/MyDrive/Colab Notebooks/lost_found_project/Scarf-1 ...\n",
            "Processing jackets from /content/drive/MyDrive/Colab Notebooks/lost_found_project/jackets-vests-1 ...\n",
            "Processing lostandfound from /content/drive/MyDrive/Colab Notebooks/lost_found_project/Lost-and-found-items-7 ...\n",
            "\n",
            "Creating validation split from training data...\n",
            "\n",
            "Dataset created at: /content/drive/MyDrive/Colab Notebooks/lost_found_project/final_dataset\n",
            "Classes (66): ['Airpods', 'c to c', 'c toc', 'lightning to c', 'usb in 3', 'usb to  LIghtning', 'usb to c', 'usb to micro-usb', 'colby student card', 'keys', 'powerbank', 'Scarf', 'jackets', 'vests', 'Backpack', 'Belt-bag', 'Car-key-Acura', 'Car-key-Chevrolet', 'Car-key-Ford', 'Car-key-Honda', 'Car-key-Hyundai', 'Car-key-Kia', 'Car-key-Lexus', 'Car-key-Mercedes-Benz', 'Car-key-Mitsubishi', 'Car-key-Nissan', 'Car-key-Toyota', 'Car-key-Volkswagen', 'Car-key-non-specific', 'Card', 'Cross-bag', 'Electronic-devices-AirPods', 'Electronic-devices-AirPods-case', 'Electronic-devices-charger', 'Electronic-devices-earphones', 'Electronic-devices-headphones', 'Electronic-devices-laptop', 'Electronic-devices-laptop-charger', 'Electronic-devices-phone', 'Electronic-devices-phone-charger', 'Electronic-devices-powerbank', 'Electronic-devices-tablet', 'Glasses', 'Handbag', 'Key', 'Laptop-bag', 'Smart-watch-black', 'Smart-watch-gold', 'Smart-watch-pink', 'Smart-watch-silver', 'Smart-watch-white', 'Sunglasses', 'Travel-bag', 'Wallet-black', 'Wallet-brown', 'Wallet-grey', 'Wallet-multicolor', 'Wallet-pink', 'Wallet-red', 'Wallet-turquoise', 'Watch-black', 'Watch-blue', 'Watch-brown', 'Watch-gold', 'Watch-silver', 'Watch-white']\n",
            "\n",
            "Train: 2466 (76.6%)\n",
            "Valid: 755 (23.4%)\n",
            "Total: 3221\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import yaml\n",
        "from collections import defaultdict\n",
        "\n",
        "dataset_path = output_path\n",
        "\n",
        "\n",
        "with open(f\"{dataset_path}/data.yaml\", \"r\") as f:\n",
        "    data = yaml.safe_load(f)\n",
        "\n",
        "classes = data[\"names\"]\n",
        "if isinstance(classes, dict):\n",
        "    classes = list(classes.values())\n",
        "\n",
        "def group_class(class_name):\n",
        "    lower_name = class_name.lower()\n",
        "\n",
        "    if \" to \" in lower_name or \"to\" in lower_name.split(\"-\") or \"cable\" in lower_name or \"toc\" in lower_name or \"usb\" in lower_name:\n",
        "        return \"cables\"\n",
        "\n",
        "    if \"car-key\" in lower_name or \"car key\" in lower_name:\n",
        "        return \"car-keys\"\n",
        "\n",
        "    if \"watch\" in lower_name:\n",
        "        if \"smart\" in lower_name:\n",
        "            return \"smart-watches\"\n",
        "        else:\n",
        "            return \"watches\"\n",
        "\n",
        "    if \"wallet\" in lower_name:\n",
        "        return \"wallets\"\n",
        "\n",
        "    if \"phone\" in lower_name:\n",
        "        return \"phones\"\n",
        "\n",
        "    if \"airpod\" in lower_name:\n",
        "        return \"airpods\"\n",
        "\n",
        "    if \"tablet\" in lower_name:\n",
        "        return \"tablets\"\n",
        "\n",
        "    if \"glasses\" in lower_name or \"sunglasses\" in lower_name:\n",
        "        return \"glasses\"\n",
        "\n",
        "    if \"glove\" in lower_name:\n",
        "        return \"gloves\"\n",
        "\n",
        "    if \"powerbank\" in lower_name:\n",
        "        return \"powerbank\"\n",
        "\n",
        "\n",
        "    if \"laptop\" in lower_name or \"charger\" in lower_name:\n",
        "        if \"charger\" in lower_name:\n",
        "            return \"chargers\"\n",
        "        else:\n",
        "            return \"laptops\"\n",
        "    if \"bag\" in lower_name:\n",
        "      return \"bags(other than backpack)\"\n",
        "\n",
        "    if \"keys\" in lower_name or \"key\" in lower_name:\n",
        "      return \"keys\"\n",
        "\n",
        "    return class_name\n",
        "\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"image distribution by grouped label\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "group_image_counts = defaultdict(int)\n",
        "total_images = 0\n",
        "\n",
        "for split in [\"train\", \"valid\", \"test\"]:\n",
        "    lbl_dir = os.path.join(dataset_path, split, \"labels\")\n",
        "    if not os.path.exists(lbl_dir):\n",
        "        continue\n",
        "\n",
        "    for lbl_file in os.listdir(lbl_dir):\n",
        "        if not lbl_file.endswith(\".txt\"):\n",
        "            continue\n",
        "\n",
        "        total_images += 1\n",
        "        img_groups = set()   # groups that appear in this image\n",
        "\n",
        "        with open(os.path.join(lbl_dir, lbl_file), \"r\") as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) < 5:\n",
        "                    continue\n",
        "                cls_id = int(parts[0])\n",
        "                if 0 <= cls_id < len(classes):\n",
        "                    cls_name = classes[cls_id]\n",
        "                    g = group_class(cls_name)\n",
        "                    img_groups.add(g)\n",
        "\n",
        "        # count this image once for each group it contains\n",
        "        for g in img_groups:\n",
        "            group_image_counts[g] += 1\n",
        "\n",
        "print(f\"\\nTotal images (train+valid+test): {total_images}\\n\")\n",
        "\n",
        "for g, count in sorted(group_image_counts.items(),\n",
        "                       key=lambda x: x[1],\n",
        "                       reverse=True):\n",
        "    pct = count / total_images * 100 if total_images > 0 else 0\n",
        "    print(f\"{g}: {count} images ({pct:.1f}%)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sl4hhM1yH1B",
        "outputId": "d281b8d4-daf9-45b9-d039-cd10e30c78a5"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "image distribution by grouped label\n",
            "============================================================\n",
            "\n",
            "Total images (train+valid+test): 3337\n",
            "\n",
            "phones: 488 images (14.6%)\n",
            "jackets: 381 images (11.4%)\n",
            "glasses: 342 images (10.2%)\n",
            "airpods: 270 images (8.1%)\n",
            "keys: 231 images (6.9%)\n",
            "Scarf: 200 images (6.0%)\n",
            "cables: 198 images (5.9%)\n",
            "watches: 197 images (5.9%)\n",
            "bags(other than backpack): 176 images (5.3%)\n",
            "wallets: 174 images (5.2%)\n",
            "Card: 135 images (4.0%)\n",
            "car-keys: 131 images (3.9%)\n",
            "Backpack: 109 images (3.3%)\n",
            "laptops: 103 images (3.1%)\n",
            "powerbank: 53 images (1.6%)\n",
            "colby student card: 45 images (1.3%)\n",
            "tablets: 39 images (1.2%)\n",
            "chargers: 37 images (1.1%)\n",
            "smart-watches: 26 images (0.8%)\n",
            "vests: 18 images (0.5%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import yaml\n",
        "\n",
        "\n",
        "dataset_path = '/content/drive/MyDrive/Colab Notebooks/lost_found_project/final_dataset'\n",
        "\n",
        "# slimmed dataset\n",
        "slim_dataset_path = dataset_path + \"_slim\"\n",
        "os.makedirs(slim_dataset_path, exist_ok=True)\n",
        "\n",
        "\n",
        "with open(os.path.join(dataset_path, \"data.yaml\"), \"r\") as f:\n",
        "    data = yaml.safe_load(f)\n",
        "\n",
        "orig_classes = data[\"names\"]\n",
        "if isinstance(orig_classes, dict):\n",
        "    orig_classes = [orig_classes[i] for i in sorted(orig_classes.keys())]\n",
        "\n",
        "\n",
        "group_names = [\n",
        "    \"phones\",\n",
        "    \"jackets\",\n",
        "    \"glasses\",\n",
        "    \"airpods\",\n",
        "    \"Scarf\",\n",
        "    \"cables\",\n",
        "    \"watches\",\n",
        "    \"bags(other than backpack)\",\n",
        "    \"wallets\",\n",
        "    \"card\",\n",
        "    \"car-keys\",\n",
        "    \"Backpack\",\n",
        "    \"laptops\",\n",
        "    \"keys\",\n",
        "    \"powerbank\",\n",
        "    \"colby student card\",\n",
        "    \"tablets\",\n",
        "    \"chargers\",\n",
        "    \"smart-watches\",\n",
        "    \"vests\",\n",
        "]\n",
        "group2id = {name: i for i, name in enumerate(group_names)}\n",
        "\n",
        "\n",
        "def group_class(class_name: str):\n",
        "    lower = class_name.lower()\n",
        "\n",
        "    # Phones, laptops, tablets\n",
        "    if \"phone\" in lower:\n",
        "        return \"phones\"\n",
        "    if \"tablet\" in lower:\n",
        "        return \"tablets\"\n",
        "    if \"laptop\" in lower:\n",
        "        return \"laptops\"\n",
        "\n",
        "    # Jackets, vests, scarves\n",
        "    if \"jacket\" in lower:\n",
        "        return \"jackets\"\n",
        "    if \"vest\" in lower:\n",
        "        return \"vests\"\n",
        "    if \"scarf\" in lower:\n",
        "        return \"Scarf\"\n",
        "\n",
        "    # Glasses & sunglasses\n",
        "    if \"glasses\" in lower or \"sunglasses\" in lower:\n",
        "        return \"glasses\"\n",
        "\n",
        "    # AirPods, earbuds, earphones\n",
        "    if \"airpod\" in lower or \"earphone\" in lower or \"earbud\" in lower:\n",
        "        return \"airpods\"\n",
        "\n",
        "    # Cables & chargers & powerbank\n",
        "    if \"usb\" in lower or \"c to c\" in lower or \"c-to-c\" in lower or \"cable\" in lower or \" to \" in lower:\n",
        "        return \"cables\"\n",
        "    if \"charger\" in lower:\n",
        "        return \"chargers\"\n",
        "    if \"powerbank\" in lower:\n",
        "        return \"powerbank\"\n",
        "\n",
        "    # Watches\n",
        "    if \"smart-watch\" in lower.replace(\"_\", \"-\"):\n",
        "        return \"smart-watches\"\n",
        "    if \"watch\" in lower:\n",
        "        return \"watches\"\n",
        "\n",
        "    # Bags and backpack\n",
        "    if \"backpack\" in lower:\n",
        "        return \"Backpack\"\n",
        "    if \"bag\" in lower:\n",
        "        return \"bags(other than backpack)\"\n",
        "\n",
        "    # Wallets\n",
        "    if \"wallet\" in lower:\n",
        "        return \"wallets\"\n",
        "\n",
        "    # Cards\n",
        "    if \"colby\" in lower and \"card\" in lower:\n",
        "        return \"colby student card\"\n",
        "    if \"card\" in lower:\n",
        "        return \"card\"\n",
        "\n",
        "    # Keys & car-keys\n",
        "    if \"car-key\" in lower or \"car key\" in lower:\n",
        "        return \"car-keys\"\n",
        "    if \"keys\" in lower or \"key\" in lower:\n",
        "        return \"keys\"\n",
        "\n",
        "    # Classes not matched will be removed\n",
        "    return None\n",
        "\n",
        "# Rewrite labels & copy images\n",
        "splits = [\"train\", \"valid\", \"test\"]\n",
        "\n",
        "for split in splits:\n",
        "    img_src_dir = os.path.join(dataset_path, split, \"images\")\n",
        "    lbl_src_dir = os.path.join(dataset_path, split, \"labels\")\n",
        "\n",
        "    if not os.path.exists(img_src_dir) or not os.path.exists(lbl_src_dir):\n",
        "        print(f\"split '{split}' not found, skipping.\")\n",
        "        continue\n",
        "\n",
        "    img_dst_dir = os.path.join(slim_dataset_path, split, \"images\")\n",
        "    lbl_dst_dir = os.path.join(slim_dataset_path, split, \"labels\")\n",
        "    os.makedirs(img_dst_dir, exist_ok=True)\n",
        "    os.makedirs(lbl_dst_dir, exist_ok=True)\n",
        "\n",
        "    for lbl_file in os.listdir(lbl_src_dir):\n",
        "        if not lbl_file.endswith(\".txt\"):\n",
        "            continue\n",
        "\n",
        "        src_lbl_path = os.path.join(lbl_src_dir, lbl_file)\n",
        "        dst_lbl_path = os.path.join(lbl_dst_dir, lbl_file)\n",
        "\n",
        "        with open(src_lbl_path, \"r\") as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        new_lines = []\n",
        "        for line in lines:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) < 5:\n",
        "                continue\n",
        "\n",
        "            cls_id = int(parts[0])\n",
        "            if not (0 <= cls_id < len(orig_classes)):\n",
        "                continue\n",
        "\n",
        "            orig_name = orig_classes[cls_id]\n",
        "            gname = group_class(orig_name)\n",
        "            if gname is None:\n",
        "                continue\n",
        "\n",
        "            if gname not in group2id:\n",
        "                print(f\"grouped name '{gname}' not in group_names (orig '{orig_name}')\")\n",
        "                continue\n",
        "\n",
        "            new_id = group2id[gname]\n",
        "            new_line = \" \".join([str(new_id)] + parts[1:])\n",
        "            new_lines.append(new_line + \"\\n\")\n",
        "\n",
        "        # Only keep images that have at least 1 valid object\n",
        "        if new_lines:\n",
        "            with open(dst_lbl_path, \"w\") as f:\n",
        "                f.writelines(new_lines)\n",
        "\n",
        "            base = os.path.splitext(lbl_file)[0]\n",
        "            found = False\n",
        "            for ext in [\".jpg\", \".jpeg\", \".png\"]:\n",
        "                src_img_path = os.path.join(img_src_dir, base + ext)\n",
        "                if os.path.exists(src_img_path):\n",
        "                    shutil.copy2(src_img_path, os.path.join(img_dst_dir, base + ext))\n",
        "                    found = True\n",
        "                    break\n",
        "            if not found:\n",
        "                print(f\"[WARN] image for {lbl_file} not found in {img_src_dir}\")\n",
        "\n",
        "print(\"Slim dataset written to:\", slim_dataset_path)\n",
        "\n",
        "# data_slim.yaml\n",
        "new_data = {\n",
        "    \"path\": slim_dataset_path,\n",
        "    \"train\": os.path.join(slim_dataset_path, \"train\", \"images\"),\n",
        "    \"val\": os.path.join(slim_dataset_path, \"valid\", \"images\"),\n",
        "    \"test\": os.path.join(slim_dataset_path, \"test\", \"images\"),\n",
        "    \"nc\": len(group_names),\n",
        "    \"names\": {i: name for i, name in enumerate(group_names)},\n",
        "}\n",
        "\n",
        "with open(os.path.join(slim_dataset_path, \"data_slim.yaml\"), \"w\") as f:\n",
        "    yaml.safe_dump(new_data, f, sort_keys=False)\n",
        "\n",
        "print(\"New data file saved to:\", os.path.join(slim_dataset_path, \"data_slim.yaml\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MxfPqV1KKMo",
        "outputId": "c38bd6ab-a6c0-4c97-9ca2-2a9080903016"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Slim dataset written to: /content/drive/MyDrive/Colab Notebooks/lost_found_project/final_dataset_slim\n",
            "New data file saved to: /content/drive/MyDrive/Colab Notebooks/lost_found_project/final_dataset_slim/data_slim.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "os.system('pip install ultralytics -q')\n",
        "\n",
        "from ultralytics import YOLO\n",
        "from IPython.display import Image, display\n",
        "\n",
        "print(\"Setup complete\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSIN8BcxQ31A",
        "outputId": "1e9f0228-3d64-4701-b531-92da1b075558"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import shutil\n",
        "from ultralytics import YOLO\n",
        "import torch\n",
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Load COCO-pretrained YOLO model nano\n",
        "model = YOLO(\"yolov8n.pt\")\n",
        "\n",
        "# Fine-tune on slimmed dataset\n",
        "results = model.train(\n",
        "    data=\"/content/drive/MyDrive/Colab Notebooks/lost_found_project/final_dataset_slim/data_slim.yaml\",\n",
        "    epochs=50,\n",
        "    imgsz=640,\n",
        "    batch=16,\n",
        "    workers=4,\n",
        "    project=\"lostfound_slim_exp\",\n",
        "    name=\"yolov8s_slim_finetune\",\n",
        "    freeze = 10\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQZGN1uyWtoU",
        "outputId": "4cbd0934-6f5a-4917-cf08-f8c4c7973d0b"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.233 ðŸš€ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/drive/MyDrive/Colab Notebooks/lost_found_project/final_dataset_slim/data_slim.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=10, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov8s_slim_finetune2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=lostfound_slim_exp, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/drive/MyDrive/Colab Notebooks/lost_found_project/lostfound_slim_exp/yolov8s_slim_finetune2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=4, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=20\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    755212  ultralytics.nn.modules.head.Detect           [20, [64, 128, 256]]          \n",
            "Model summary: 129 layers, 3,014,748 parameters, 3,014,732 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.0.conv.weight'\n",
            "Freezing layer 'model.0.bn.weight'\n",
            "Freezing layer 'model.0.bn.bias'\n",
            "Freezing layer 'model.1.conv.weight'\n",
            "Freezing layer 'model.1.bn.weight'\n",
            "Freezing layer 'model.1.bn.bias'\n",
            "Freezing layer 'model.2.cv1.conv.weight'\n",
            "Freezing layer 'model.2.cv1.bn.weight'\n",
            "Freezing layer 'model.2.cv1.bn.bias'\n",
            "Freezing layer 'model.2.cv2.conv.weight'\n",
            "Freezing layer 'model.2.cv2.bn.weight'\n",
            "Freezing layer 'model.2.cv2.bn.bias'\n",
            "Freezing layer 'model.2.m.0.cv1.conv.weight'\n",
            "Freezing layer 'model.2.m.0.cv1.bn.weight'\n",
            "Freezing layer 'model.2.m.0.cv1.bn.bias'\n",
            "Freezing layer 'model.2.m.0.cv2.conv.weight'\n",
            "Freezing layer 'model.2.m.0.cv2.bn.weight'\n",
            "Freezing layer 'model.2.m.0.cv2.bn.bias'\n",
            "Freezing layer 'model.3.conv.weight'\n",
            "Freezing layer 'model.3.bn.weight'\n",
            "Freezing layer 'model.3.bn.bias'\n",
            "Freezing layer 'model.4.cv1.conv.weight'\n",
            "Freezing layer 'model.4.cv1.bn.weight'\n",
            "Freezing layer 'model.4.cv1.bn.bias'\n",
            "Freezing layer 'model.4.cv2.conv.weight'\n",
            "Freezing layer 'model.4.cv2.bn.weight'\n",
            "Freezing layer 'model.4.cv2.bn.bias'\n",
            "Freezing layer 'model.4.m.0.cv1.conv.weight'\n",
            "Freezing layer 'model.4.m.0.cv1.bn.weight'\n",
            "Freezing layer 'model.4.m.0.cv1.bn.bias'\n",
            "Freezing layer 'model.4.m.0.cv2.conv.weight'\n",
            "Freezing layer 'model.4.m.0.cv2.bn.weight'\n",
            "Freezing layer 'model.4.m.0.cv2.bn.bias'\n",
            "Freezing layer 'model.4.m.1.cv1.conv.weight'\n",
            "Freezing layer 'model.4.m.1.cv1.bn.weight'\n",
            "Freezing layer 'model.4.m.1.cv1.bn.bias'\n",
            "Freezing layer 'model.4.m.1.cv2.conv.weight'\n",
            "Freezing layer 'model.4.m.1.cv2.bn.weight'\n",
            "Freezing layer 'model.4.m.1.cv2.bn.bias'\n",
            "Freezing layer 'model.5.conv.weight'\n",
            "Freezing layer 'model.5.bn.weight'\n",
            "Freezing layer 'model.5.bn.bias'\n",
            "Freezing layer 'model.6.cv1.conv.weight'\n",
            "Freezing layer 'model.6.cv1.bn.weight'\n",
            "Freezing layer 'model.6.cv1.bn.bias'\n",
            "Freezing layer 'model.6.cv2.conv.weight'\n",
            "Freezing layer 'model.6.cv2.bn.weight'\n",
            "Freezing layer 'model.6.cv2.bn.bias'\n",
            "Freezing layer 'model.6.m.0.cv1.conv.weight'\n",
            "Freezing layer 'model.6.m.0.cv1.bn.weight'\n",
            "Freezing layer 'model.6.m.0.cv1.bn.bias'\n",
            "Freezing layer 'model.6.m.0.cv2.conv.weight'\n",
            "Freezing layer 'model.6.m.0.cv2.bn.weight'\n",
            "Freezing layer 'model.6.m.0.cv2.bn.bias'\n",
            "Freezing layer 'model.6.m.1.cv1.conv.weight'\n",
            "Freezing layer 'model.6.m.1.cv1.bn.weight'\n",
            "Freezing layer 'model.6.m.1.cv1.bn.bias'\n",
            "Freezing layer 'model.6.m.1.cv2.conv.weight'\n",
            "Freezing layer 'model.6.m.1.cv2.bn.weight'\n",
            "Freezing layer 'model.6.m.1.cv2.bn.bias'\n",
            "Freezing layer 'model.7.conv.weight'\n",
            "Freezing layer 'model.7.bn.weight'\n",
            "Freezing layer 'model.7.bn.bias'\n",
            "Freezing layer 'model.8.cv1.conv.weight'\n",
            "Freezing layer 'model.8.cv1.bn.weight'\n",
            "Freezing layer 'model.8.cv1.bn.bias'\n",
            "Freezing layer 'model.8.cv2.conv.weight'\n",
            "Freezing layer 'model.8.cv2.bn.weight'\n",
            "Freezing layer 'model.8.cv2.bn.bias'\n",
            "Freezing layer 'model.8.m.0.cv1.conv.weight'\n",
            "Freezing layer 'model.8.m.0.cv1.bn.weight'\n",
            "Freezing layer 'model.8.m.0.cv1.bn.bias'\n",
            "Freezing layer 'model.8.m.0.cv2.conv.weight'\n",
            "Freezing layer 'model.8.m.0.cv2.bn.weight'\n",
            "Freezing layer 'model.8.m.0.cv2.bn.bias'\n",
            "Freezing layer 'model.9.cv1.conv.weight'\n",
            "Freezing layer 'model.9.cv1.bn.weight'\n",
            "Freezing layer 'model.9.cv1.bn.bias'\n",
            "Freezing layer 'model.9.cv2.conv.weight'\n",
            "Freezing layer 'model.9.cv2.bn.weight'\n",
            "Freezing layer 'model.9.cv2.bn.bias'\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 3.1Â±5.1 ms, read: 12.8Â±7.7 MB/s, size: 31.1 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Colab Notebooks/lost_found_project/final_dataset_slim/train/labels.cache... 2463 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2463/2463 3.3Mit/s 0.0s\n",
            "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 294, len(boxes) = 2751. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 1.1Â±1.1 ms, read: 17.6Â±6.7 MB/s, size: 47.8 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Colab Notebooks/lost_found_project/final_dataset_slim/valid/labels.cache... 755 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 755/755 581.2Kit/s 0.0s\n",
            "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 20, len(boxes) = 813. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
            "Plotting labels to /content/drive/MyDrive/Colab Notebooks/lost_found_project/lostfound_slim_exp/yolov8s_slim_finetune2/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000417, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/drive/MyDrive/Colab Notebooks/lost_found_project/lostfound_slim_exp/yolov8s_slim_finetune2\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/50      2.81G     0.9209      3.623       1.42         36        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.1it/s 50.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 2.6it/s 9.1s\n",
            "                   all        755        813      0.609      0.287      0.262      0.182\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/50      2.81G     0.9225      2.616      1.414         29        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.4it/s 46.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 2.6it/s 9.4s\n",
            "                   all        755        813      0.674      0.286      0.345      0.223\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/50      2.81G     0.9143      2.325       1.38         37        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.4it/s 45.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 2.8it/s 8.6s\n",
            "                   all        755        813      0.677      0.386       0.41      0.281\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/50      2.81G     0.8859      2.065      1.345         40        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.3it/s 46.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 2.8it/s 8.5s\n",
            "                   all        755        813      0.675      0.434      0.466       0.31\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/50      2.81G      0.874      1.886      1.332         37        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.3it/s 46.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.0it/s 8.1s\n",
            "                   all        755        813      0.626      0.437      0.497      0.341\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/50      2.81G     0.8429      1.774      1.314         37        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.3it/s 47.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.1it/s 7.8s\n",
            "                   all        755        813      0.754      0.469      0.539      0.384\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/50      2.81G     0.8294      1.659      1.301         39        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.2it/s 48.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 2.5it/s 9.5s\n",
            "                   all        755        813      0.686      0.508       0.53      0.375\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/50      2.81G     0.8157      1.581      1.282         37        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.3it/s 46.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 2.8it/s 8.5s\n",
            "                   all        755        813      0.734      0.513      0.574      0.395\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/50      2.81G     0.7913      1.491      1.267         34        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.3it/s 47.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.0it/s 8.1s\n",
            "                   all        755        813      0.703      0.513      0.581       0.42\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/50      2.81G     0.7807      1.428       1.25         43        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.3it/s 46.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 2.7it/s 9.0s\n",
            "                   all        755        813      0.706      0.533      0.595      0.424\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      11/50      2.81G     0.7909      1.373      1.264         43        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.3it/s 46.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.0it/s 8.0s\n",
            "                   all        755        813      0.728      0.557      0.608      0.434\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      12/50      2.81G     0.7667      1.323      1.239         37        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.3it/s 46.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.2it/s 7.5s\n",
            "                   all        755        813      0.706       0.53      0.584      0.423\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      13/50      2.81G     0.7621      1.303      1.237         35        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.3it/s 46.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.0it/s 8.0s\n",
            "                   all        755        813      0.698      0.606      0.644      0.476\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      14/50      2.81G     0.7576      1.261      1.239         42        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.2it/s 48.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.2it/s 7.5s\n",
            "                   all        755        813      0.631      0.622      0.611      0.438\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      15/50      2.81G     0.7493      1.193      1.226         51        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.3it/s 47.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.2it/s 7.4s\n",
            "                   all        755        813       0.71      0.621      0.668      0.478\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      16/50      2.81G     0.7407      1.191      1.226         37        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.2it/s 47.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.5it/s 6.8s\n",
            "                   all        755        813      0.715      0.645      0.651      0.474\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      17/50      2.81G     0.7314      1.162       1.22         32        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.3it/s 47.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.2it/s 7.5s\n",
            "                   all        755        813      0.711      0.596      0.639      0.474\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      18/50      2.81G      0.733      1.133      1.216         29        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.3it/s 47.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.3it/s 7.2s\n",
            "                   all        755        813      0.757      0.618       0.68      0.498\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      19/50      2.81G     0.7066      1.072      1.204         42        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.2it/s 47.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.4it/s 7.0s\n",
            "                   all        755        813      0.744      0.619      0.661      0.481\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      20/50      2.81G     0.7106       1.08      1.202         36        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.2it/s 48.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.2it/s 7.4s\n",
            "                   all        755        813      0.589      0.654      0.654      0.495\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      21/50      2.81G     0.7171      1.059      1.202         38        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.1it/s 49.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.2it/s 7.5s\n",
            "                   all        755        813      0.764      0.624      0.682      0.518\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      22/50      2.81G     0.6854      1.025      1.192         48        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.2it/s 48.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.4it/s 7.0s\n",
            "                   all        755        813      0.716      0.689      0.697      0.529\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      23/50      2.81G     0.6846       1.01      1.186         32        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.2it/s 47.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.2it/s 7.6s\n",
            "                   all        755        813      0.723      0.644      0.691      0.516\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      24/50      2.81G     0.6835     0.9993       1.18         39        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.3it/s 47.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.1it/s 7.8s\n",
            "                   all        755        813      0.713      0.669      0.679      0.515\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      25/50      2.81G     0.6727     0.9753      1.179         46        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.3it/s 47.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.3it/s 7.3s\n",
            "                   all        755        813      0.708      0.677      0.689      0.516\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      26/50      2.81G     0.6684     0.9761      1.172         29        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.2it/s 48.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.1it/s 7.7s\n",
            "                   all        755        813      0.764      0.664      0.695      0.518\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      27/50      2.81G      0.659     0.9432      1.168         30        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.3it/s 46.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 2.9it/s 8.3s\n",
            "                   all        755        813      0.731      0.653      0.692      0.527\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      28/50      2.81G     0.6462     0.9086      1.155         30        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.1it/s 50.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.3it/s 7.2s\n",
            "                   all        755        813      0.731      0.661      0.677      0.512\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      29/50      2.81G     0.6585      0.903      1.166         37        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.3it/s 46.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 2.9it/s 8.4s\n",
            "                   all        755        813      0.729      0.658      0.707      0.549\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      30/50      2.81G     0.6444     0.8804      1.148         33        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.2it/s 47.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.3it/s 7.2s\n",
            "                   all        755        813      0.737      0.616      0.697      0.532\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      31/50      2.81G     0.6492     0.8911      1.161         41        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.2it/s 47.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.0it/s 7.9s\n",
            "                   all        755        813      0.732      0.672      0.701      0.535\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      32/50      2.81G     0.6426     0.8596      1.149         38        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.4it/s 45.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 2.7it/s 9.0s\n",
            "                   all        755        813      0.772      0.666      0.711      0.548\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      33/50      2.81G     0.6345     0.8619      1.145         42        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.3it/s 46.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 2.9it/s 8.3s\n",
            "                   all        755        813      0.808      0.668       0.73      0.555\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      34/50      2.81G      0.636     0.8395      1.144         39        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.3it/s 47.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.0it/s 8.0s\n",
            "                   all        755        813      0.735      0.732      0.734       0.56\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      35/50      2.81G     0.6164     0.8201      1.138         38        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.2it/s 47.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 2.9it/s 8.4s\n",
            "                   all        755        813      0.754      0.721      0.742      0.564\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      36/50      2.81G     0.6243     0.8314      1.135         36        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.3it/s 46.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 2.8it/s 8.6s\n",
            "                   all        755        813      0.785      0.667      0.737      0.567\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      37/50      2.81G     0.6243     0.8289      1.137         48        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.3it/s 46.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 2.9it/s 8.4s\n",
            "                   all        755        813      0.731      0.711      0.731      0.568\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      38/50      2.81G     0.6148     0.8024      1.137         25        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.4it/s 45.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 2.9it/s 8.2s\n",
            "                   all        755        813      0.737      0.724      0.743      0.564\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      39/50      2.81G     0.6115     0.8129      1.138         31        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.3it/s 46.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 2.8it/s 8.5s\n",
            "                   all        755        813      0.745      0.724      0.735      0.563\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      40/50      2.81G      0.602      0.777       1.13         46        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.3it/s 46.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 2.8it/s 8.6s\n",
            "                   all        755        813      0.785      0.681      0.734      0.567\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      41/50      2.81G     0.5347     0.7003      1.128         15        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.3it/s 46.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.1it/s 7.7s\n",
            "                   all        755        813      0.805      0.693      0.741      0.587\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      42/50      2.81G     0.5229     0.6451      1.117         20        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.4it/s 44.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 2.8it/s 8.5s\n",
            "                   all        755        813      0.793       0.73      0.761      0.602\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      43/50      2.81G     0.5116     0.6129      1.103         15        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.4it/s 45.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.5it/s 6.8s\n",
            "                   all        755        813      0.741      0.748      0.758      0.597\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      44/50      2.81G     0.4995     0.6083      1.092         15        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.5it/s 44.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.0it/s 8.0s\n",
            "                   all        755        813      0.807      0.738      0.771      0.616\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      45/50      2.81G     0.4974     0.5933      1.092         19        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.5it/s 44.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.1it/s 7.7s\n",
            "                   all        755        813      0.797      0.718      0.771      0.616\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      46/50      2.81G     0.4926     0.5805      1.088         16        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.5it/s 44.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 2.9it/s 8.4s\n",
            "                   all        755        813      0.777      0.753      0.789      0.628\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      47/50      2.81G      0.484     0.5674      1.082         16        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.5it/s 43.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.3it/s 7.3s\n",
            "                   all        755        813      0.808      0.738      0.779      0.625\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      48/50      2.81G     0.4832     0.5532      1.075         16        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.4it/s 45.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.2it/s 7.4s\n",
            "                   all        755        813      0.825      0.735      0.783      0.624\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      49/50      2.81G     0.4771     0.5543      1.088         15        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.6it/s 43.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 2.8it/s 8.6s\n",
            "                   all        755        813      0.826       0.73      0.778      0.624\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      50/50      2.81G     0.4747      0.539      1.072         17        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 3.5it/s 43.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.0it/s 7.9s\n",
            "                   all        755        813      0.808      0.749      0.783       0.63\n",
            "\n",
            "50 epochs completed in 0.770 hours.\n",
            "Optimizer stripped from /content/drive/MyDrive/Colab Notebooks/lost_found_project/lostfound_slim_exp/yolov8s_slim_finetune2/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from /content/drive/MyDrive/Colab Notebooks/lost_found_project/lostfound_slim_exp/yolov8s_slim_finetune2/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating /content/drive/MyDrive/Colab Notebooks/lost_found_project/lostfound_slim_exp/yolov8s_slim_finetune2/weights/best.pt...\n",
            "Ultralytics 8.3.233 ðŸš€ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,009,548 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 2.8it/s 8.5s\n",
            "                   all        755        813      0.808       0.75      0.783       0.63\n",
            "                phones        107        108       0.87      0.852      0.933      0.849\n",
            "               jackets        151        151      0.983      0.967      0.994      0.873\n",
            "               glasses         67         68      0.905      0.941      0.963      0.832\n",
            "               airpods         52         74      0.895      0.865      0.842      0.513\n",
            "                 Scarf         30         30      0.933      0.923      0.952      0.787\n",
            "                cables         41         41      0.975          1      0.995      0.774\n",
            "               watches         47         67      0.874      0.836      0.912      0.718\n",
            "bags(other than backpack)         37         37      0.769       0.73        0.8      0.644\n",
            "               wallets         34         34      0.736      0.824      0.831      0.694\n",
            "                  card         35         35          1      0.915      0.978      0.888\n",
            "              car-keys         26         29      0.774      0.793      0.856      0.755\n",
            "              Backpack         20         20      0.726        0.7      0.805      0.676\n",
            "               laptops         26         27      0.709      0.741        0.7      0.532\n",
            "                  keys         44         52      0.818      0.712      0.815      0.615\n",
            "             powerbank         11         11       0.68      0.582      0.705      0.637\n",
            "    colby student card          5          5       0.71          1      0.995      0.535\n",
            "               tablets          9          9       0.75      0.667      0.702      0.531\n",
            "              chargers          1          1          1          0          0          0\n",
            "         smart-watches          9          9      0.507      0.222      0.352      0.316\n",
            "                 vests          5          5      0.547      0.732       0.53      0.423\n",
            "Speed: 0.3ms preprocess, 2.2ms inference, 0.0ms loss, 2.6ms postprocess per image\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/Colab Notebooks/lost_found_project/lostfound_slim_exp/yolov8s_slim_finetune2\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load best model\n",
        "MODEL_WEIGHTS = \"/content/drive/MyDrive/Colab Notebooks/lost_found_project/lostfound_slim_exp/yolov8s_slim_finetune2/weights/best.pt\"\n",
        "best_model = YOLO(MODEL_WEIGHTS)"
      ],
      "metadata": {
        "id": "lUskn4hAYs46"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = best_model.val()\n",
        "\n",
        "# Print metrics\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"VALIDATION METRICS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"mAP50:     {metrics.box.map50:.3f} )\")\n",
        "print(f\"mAP50-95:  {metrics.box.map:.3f}\")\n",
        "print(f\"Precision: {metrics.box.mp:.3f}\")\n",
        "print(f\"Recall:    {metrics.box.mr:.3f}\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgx_XITdaE6c",
        "outputId": "0453bf30-a197-4d1f-b016-35e64eab51c5"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.233 ðŸš€ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.8Â±0.2 ms, read: 24.5Â±10.6 MB/s, size: 38.8 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Colab Notebooks/lost_found_project/final_dataset_slim/valid/labels.cache... 755 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 755/755 969.9Kit/s 0.0s\n",
            "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 20, len(boxes) = 813. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 48/48 4.2it/s 11.4s\n",
            "                   all        755        813      0.807       0.75      0.783       0.63\n",
            "                phones        107        108      0.866      0.852      0.934      0.851\n",
            "               jackets        151        151      0.982      0.967      0.994      0.873\n",
            "               glasses         67         68      0.905      0.941      0.963      0.832\n",
            "               airpods         52         74      0.894      0.865      0.843      0.517\n",
            "                 Scarf         30         30      0.932       0.92      0.952      0.786\n",
            "                cables         41         41      0.975          1      0.995      0.774\n",
            "               watches         47         67      0.872      0.836      0.911      0.715\n",
            "bags(other than backpack)         37         37      0.771      0.728        0.8      0.644\n",
            "               wallets         34         34      0.736      0.824      0.827       0.69\n",
            "                  card         35         35          1      0.916      0.978      0.888\n",
            "              car-keys         26         29      0.776      0.793      0.857      0.755\n",
            "              Backpack         20         20      0.735        0.7      0.806      0.677\n",
            "               laptops         26         27      0.707      0.741        0.7      0.534\n",
            "                  keys         44         52      0.817      0.712      0.815      0.617\n",
            "             powerbank         11         11       0.68      0.582      0.705      0.637\n",
            "    colby student card          5          5      0.709          1      0.995      0.535\n",
            "               tablets          9          9      0.746      0.667      0.702      0.528\n",
            "              chargers          1          1          1          0          0          0\n",
            "         smart-watches          9          9      0.496      0.222      0.355      0.319\n",
            "                 vests          5          5      0.547      0.732       0.53      0.423\n",
            "Speed: 1.9ms preprocess, 3.8ms inference, 0.0ms loss, 2.0ms postprocess per image\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/Colab Notebooks/lost_found_project/runs/detect/val7\u001b[0m\n",
            "\n",
            "============================================================\n",
            "VALIDATION METRICS\n",
            "============================================================\n",
            "mAP50:     0.783 )\n",
            "mAP50-95:  0.630\n",
            "Precision: 0.807\n",
            "Recall:    0.750\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import yaml\n",
        "from ultralytics import YOLO\n",
        "\n",
        "\n",
        "# Original slimmed dataset (with GT labels)\n",
        "DATASET_SLIM = \"/content/drive/MyDrive/Colab Notebooks/lost_found_project/final_dataset_slim\"\n",
        "DATA_YAML_SLIM = os.path.join(DATASET_SLIM, \"data_slim.yaml\")\n",
        "\n",
        "# New dataset root for distillation (teacher pseudo-labels on train)\n",
        "DATASET_DISTILL = \"/content/drive/MyDrive/Colab Notebooks/lost_found_project/final_dataset_slim_distill\"\n",
        "os.makedirs(DATASET_DISTILL, exist_ok=True)\n",
        "\n",
        "\n",
        "TEACHER_WEIGHTS = \"/content/drive/MyDrive/Colab Notebooks/lost_found_project/lostfound_slim_exp/yolov8s_slim_finetune2/weights/best.pt\"\n",
        "\n",
        "print(\"Using original slim dataset:\", DATASET_SLIM)\n",
        "print(\"Distilled dataset will be at:\", DATASET_DISTILL)\n",
        "\n",
        "# READ CLASS INFO FROM ORIGINAL SLIM YAML\n",
        "with open(DATA_YAML_SLIM, \"r\") as f:\n",
        "    cfg = yaml.safe_load(f)\n",
        "\n",
        "names = cfg[\"names\"]\n",
        "if isinstance(names, dict):\n",
        "    names = [names[i] for i in sorted(names.keys())]\n",
        "nc = len(names)\n",
        "\n",
        "print(\"\\nSlim dataset nc =\", nc)\n",
        "print(\"Classes:\")\n",
        "for i, n in enumerate(names):\n",
        "    print(f\"  {i}: {n}\")\n",
        "\n",
        "# COPY VAL / TEST AS-IS\n",
        "for split in [\"valid\", \"val\", \"test\"]:\n",
        "    src_img_dir = os.path.join(DATASET_SLIM, split, \"images\")\n",
        "    src_lbl_dir = os.path.join(DATASET_SLIM, split, \"labels\")\n",
        "    if not os.path.exists(src_img_dir):\n",
        "        continue\n",
        "\n",
        "    dst_img_dir = os.path.join(DATASET_DISTILL, split, \"images\")\n",
        "    dst_lbl_dir = os.path.join(DATASET_DISTILL, split, \"labels\")\n",
        "    os.makedirs(dst_img_dir, exist_ok=True)\n",
        "    os.makedirs(dst_lbl_dir, exist_ok=True)\n",
        "\n",
        "    # copy images\n",
        "    for img_file in os.listdir(src_img_dir):\n",
        "        src = os.path.join(src_img_dir, img_file)\n",
        "        dst = os.path.join(dst_img_dir, img_file)\n",
        "        shutil.copy2(src, dst)\n",
        "\n",
        "    # copy labels if exist\n",
        "    if os.path.exists(src_lbl_dir):\n",
        "        for lbl_file in os.listdir(src_lbl_dir):\n",
        "            if not lbl_file.endswith(\".txt\"):\n",
        "                continue\n",
        "            src = os.path.join(src_lbl_dir, lbl_file)\n",
        "            dst = os.path.join(dst_lbl_dir, lbl_file)\n",
        "            shutil.copy2(src, dst)\n",
        "\n",
        "print(\"\\nValidation/test splits copied from slim dataset.\")\n",
        "\n",
        "# GENERATE PSEUDO-LABELS FOR TRAIN USING TEACHER\n",
        "train_img_src_dir = os.path.join(DATASET_SLIM, \"train\", \"images\")\n",
        "train_img_dst_dir = os.path.join(DATASET_DISTILL, \"train\", \"images\")\n",
        "train_lbl_dst_dir = os.path.join(DATASET_DISTILL, \"train\", \"labels\")\n",
        "\n",
        "os.makedirs(train_img_dst_dir, exist_ok=True)\n",
        "os.makedirs(train_lbl_dst_dir, exist_ok=True)\n",
        "\n",
        "# Copy train images to distilled dataset\n",
        "for img_file in os.listdir(train_img_src_dir):\n",
        "    src = os.path.join(train_img_src_dir, img_file)\n",
        "    dst = os.path.join(train_img_dst_dir, img_file)\n",
        "    shutil.copy2(src, dst)\n",
        "\n",
        "# Use teacher to predict on each train image and save pseudo-label YOLO txt\n",
        "image_paths = sorted(\n",
        "    [\n",
        "        os.path.join(train_img_dst_dir, f)\n",
        "        for f in os.listdir(train_img_dst_dir)\n",
        "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"\\nGenerating pseudo-labels from teacher on train images...\")\n",
        "print(f\"Total train images: {len(image_paths)}\")\n",
        "\n",
        "\n",
        "for img_path in image_paths:\n",
        "    results = best_model.predict(\n",
        "        source=img_path,\n",
        "        conf=0.5,       # you can adjust this threshold\n",
        "        imgsz=640,\n",
        "        verbose=False\n",
        "    )[0]\n",
        "\n",
        "    # Results: boxes in absolute coords; we'll convert to normalized xywh\n",
        "    boxes = results.boxes\n",
        "    if boxes is None or len(boxes) == 0:\n",
        "        # no detections: no label file created (image treated as background)\n",
        "        continue\n",
        "\n",
        "    # Get image size from results\n",
        "    h, w = results.orig_shape\n",
        "\n",
        "    # Prepare label lines\n",
        "    label_lines = []\n",
        "    for box in boxes:\n",
        "        cls_id = int(box.cls[0].item())\n",
        "        if cls_id < 0 or cls_id >= nc:\n",
        "            # should not happen if teacher trained on same nc\n",
        "            continue\n",
        "\n",
        "        # xyxy in absolute pixels\n",
        "        x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
        "        # convert to normalized xywh\n",
        "        x_center = (x1 + x2) / 2.0 / w\n",
        "        y_center = (y1 + y2) / 2.0 / h\n",
        "        bw = (x2 - x1) / w\n",
        "        bh = (y2 - y1) / h\n",
        "\n",
        "        label_lines.append(f\"{cls_id} {x_center:.6f} {y_center:.6f} {bw:.6f} {bh:.6f}\\n\")\n",
        "\n",
        "    # Write pseudo label file if there is at least one detection\n",
        "    if label_lines:\n",
        "        base = os.path.splitext(os.path.basename(img_path))[0]\n",
        "        lbl_path = os.path.join(train_lbl_dst_dir, base + \".txt\")\n",
        "        with open(lbl_path, \"w\") as f:\n",
        "            f.writelines(label_lines)\n",
        "\n",
        "print(\"Pseudo-label generation done.\")\n",
        "\n",
        "# WRITE NEW data_distill.yaml\n",
        "DATA_YAML_DISTILL = os.path.join(DATASET_DISTILL, \"data_distill.yaml\")\n",
        "\n",
        "data_distill = {\n",
        "    \"path\": DATASET_DISTILL,\n",
        "    \"train\": os.path.join(DATASET_DISTILL, \"train\", \"images\"),\n",
        "    \"val\": os.path.join(DATASET_DISTILL, \"valid\", \"images\"),\n",
        "    \"test\": os.path.join(DATASET_DISTILL, \"test\", \"images\"),\n",
        "    \"nc\": nc,\n",
        "    \"names\": {i: name for i, name in enumerate(names)},\n",
        "}\n",
        "\n",
        "with open(DATA_YAML_DISTILL, \"w\") as f:\n",
        "    yaml.safe_dump(data_distill, f, sort_keys=False)\n",
        "\n",
        "print(\"\\nDistilled data yaml saved to:\", DATA_YAML_DISTILL)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9izdxkVf76G",
        "outputId": "1077fb79-dbac-45f9-9da5-5b9385a13015"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using original slim dataset: /content/drive/MyDrive/Colab Notebooks/lost_found_project/final_dataset_slim\n",
            "Distilled dataset will be at: /content/drive/MyDrive/Colab Notebooks/lost_found_project/final_dataset_slim_distill\n",
            "\n",
            "Slim dataset nc = 20\n",
            "Classes:\n",
            "  0: phones\n",
            "  1: jackets\n",
            "  2: glasses\n",
            "  3: airpods\n",
            "  4: Scarf\n",
            "  5: cables\n",
            "  6: watches\n",
            "  7: bags(other than backpack)\n",
            "  8: wallets\n",
            "  9: card\n",
            "  10: car-keys\n",
            "  11: Backpack\n",
            "  12: laptops\n",
            "  13: keys\n",
            "  14: powerbank\n",
            "  15: colby student card\n",
            "  16: tablets\n",
            "  17: chargers\n",
            "  18: smart-watches\n",
            "  19: vests\n",
            "\n",
            "Validation/test splits copied from slim dataset.\n",
            "\n",
            "Generating pseudo-labels from teacher on train images...\n",
            "Total train images: 2463\n",
            "Pseudo-label generation done.\n",
            "\n",
            "Distilled data yaml saved to: /content/drive/MyDrive/Colab Notebooks/lost_found_project/final_dataset_slim_distill/data_distill.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from ultralytics import YOLO\n",
        "import torch\n",
        "\n",
        "student = YOLO(\"yolov8n.pt\")\n",
        "DATA_YAML_DISTILL = \"/content/drive/MyDrive/Colab Notebooks/lost_found_project/final_dataset_slim_distill/data_distill.yaml\"\n",
        "print(\"\\nTraining student model on distilled dataset...\")\n",
        "student_results = student.train(\n",
        "    data=DATA_YAML_DISTILL,\n",
        "    epochs=40,\n",
        "    imgsz=480,\n",
        "    batch=16,\n",
        "    workers=4,\n",
        "    project=\"lostfound_slim_exp\",\n",
        "    name=\"yolov8n_student_distilled\",\n",
        "    freeze=10,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BmMuWhsgg27",
        "outputId": "b874a0fe-aa80-4531-d426-a9d34c66583b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training student model on distilled dataset...\n",
            "Ultralytics 8.3.234 ðŸš€ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/drive/MyDrive/Colab Notebooks/lost_found_project/final_dataset_slim_distill/data_distill.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=40, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=10, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=480, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov8n_student_distilled3, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=lostfound_slim_exp, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/lostfound_slim_exp/yolov8n_student_distilled3, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=4, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=20\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    755212  ultralytics.nn.modules.head.Detect           [20, [64, 128, 256]]          \n",
            "Model summary: 129 layers, 3,014,748 parameters, 3,014,732 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.0.conv.weight'\n",
            "Freezing layer 'model.0.bn.weight'\n",
            "Freezing layer 'model.0.bn.bias'\n",
            "Freezing layer 'model.1.conv.weight'\n",
            "Freezing layer 'model.1.bn.weight'\n",
            "Freezing layer 'model.1.bn.bias'\n",
            "Freezing layer 'model.2.cv1.conv.weight'\n",
            "Freezing layer 'model.2.cv1.bn.weight'\n",
            "Freezing layer 'model.2.cv1.bn.bias'\n",
            "Freezing layer 'model.2.cv2.conv.weight'\n",
            "Freezing layer 'model.2.cv2.bn.weight'\n",
            "Freezing layer 'model.2.cv2.bn.bias'\n",
            "Freezing layer 'model.2.m.0.cv1.conv.weight'\n",
            "Freezing layer 'model.2.m.0.cv1.bn.weight'\n",
            "Freezing layer 'model.2.m.0.cv1.bn.bias'\n",
            "Freezing layer 'model.2.m.0.cv2.conv.weight'\n",
            "Freezing layer 'model.2.m.0.cv2.bn.weight'\n",
            "Freezing layer 'model.2.m.0.cv2.bn.bias'\n",
            "Freezing layer 'model.3.conv.weight'\n",
            "Freezing layer 'model.3.bn.weight'\n",
            "Freezing layer 'model.3.bn.bias'\n",
            "Freezing layer 'model.4.cv1.conv.weight'\n",
            "Freezing layer 'model.4.cv1.bn.weight'\n",
            "Freezing layer 'model.4.cv1.bn.bias'\n",
            "Freezing layer 'model.4.cv2.conv.weight'\n",
            "Freezing layer 'model.4.cv2.bn.weight'\n",
            "Freezing layer 'model.4.cv2.bn.bias'\n",
            "Freezing layer 'model.4.m.0.cv1.conv.weight'\n",
            "Freezing layer 'model.4.m.0.cv1.bn.weight'\n",
            "Freezing layer 'model.4.m.0.cv1.bn.bias'\n",
            "Freezing layer 'model.4.m.0.cv2.conv.weight'\n",
            "Freezing layer 'model.4.m.0.cv2.bn.weight'\n",
            "Freezing layer 'model.4.m.0.cv2.bn.bias'\n",
            "Freezing layer 'model.4.m.1.cv1.conv.weight'\n",
            "Freezing layer 'model.4.m.1.cv1.bn.weight'\n",
            "Freezing layer 'model.4.m.1.cv1.bn.bias'\n",
            "Freezing layer 'model.4.m.1.cv2.conv.weight'\n",
            "Freezing layer 'model.4.m.1.cv2.bn.weight'\n",
            "Freezing layer 'model.4.m.1.cv2.bn.bias'\n",
            "Freezing layer 'model.5.conv.weight'\n",
            "Freezing layer 'model.5.bn.weight'\n",
            "Freezing layer 'model.5.bn.bias'\n",
            "Freezing layer 'model.6.cv1.conv.weight'\n",
            "Freezing layer 'model.6.cv1.bn.weight'\n",
            "Freezing layer 'model.6.cv1.bn.bias'\n",
            "Freezing layer 'model.6.cv2.conv.weight'\n",
            "Freezing layer 'model.6.cv2.bn.weight'\n",
            "Freezing layer 'model.6.cv2.bn.bias'\n",
            "Freezing layer 'model.6.m.0.cv1.conv.weight'\n",
            "Freezing layer 'model.6.m.0.cv1.bn.weight'\n",
            "Freezing layer 'model.6.m.0.cv1.bn.bias'\n",
            "Freezing layer 'model.6.m.0.cv2.conv.weight'\n",
            "Freezing layer 'model.6.m.0.cv2.bn.weight'\n",
            "Freezing layer 'model.6.m.0.cv2.bn.bias'\n",
            "Freezing layer 'model.6.m.1.cv1.conv.weight'\n",
            "Freezing layer 'model.6.m.1.cv1.bn.weight'\n",
            "Freezing layer 'model.6.m.1.cv1.bn.bias'\n",
            "Freezing layer 'model.6.m.1.cv2.conv.weight'\n",
            "Freezing layer 'model.6.m.1.cv2.bn.weight'\n",
            "Freezing layer 'model.6.m.1.cv2.bn.bias'\n",
            "Freezing layer 'model.7.conv.weight'\n",
            "Freezing layer 'model.7.bn.weight'\n",
            "Freezing layer 'model.7.bn.bias'\n",
            "Freezing layer 'model.8.cv1.conv.weight'\n",
            "Freezing layer 'model.8.cv1.bn.weight'\n",
            "Freezing layer 'model.8.cv1.bn.bias'\n",
            "Freezing layer 'model.8.cv2.conv.weight'\n",
            "Freezing layer 'model.8.cv2.bn.weight'\n",
            "Freezing layer 'model.8.cv2.bn.bias'\n",
            "Freezing layer 'model.8.m.0.cv1.conv.weight'\n",
            "Freezing layer 'model.8.m.0.cv1.bn.weight'\n",
            "Freezing layer 'model.8.m.0.cv1.bn.bias'\n",
            "Freezing layer 'model.8.m.0.cv2.conv.weight'\n",
            "Freezing layer 'model.8.m.0.cv2.bn.weight'\n",
            "Freezing layer 'model.8.m.0.cv2.bn.bias'\n",
            "Freezing layer 'model.9.cv1.conv.weight'\n",
            "Freezing layer 'model.9.cv1.bn.weight'\n",
            "Freezing layer 'model.9.cv1.bn.bias'\n",
            "Freezing layer 'model.9.cv2.conv.weight'\n",
            "Freezing layer 'model.9.cv2.bn.weight'\n",
            "Freezing layer 'model.9.cv2.bn.bias'\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.7Â±0.3 ms, read: 14.6Â±11.1 MB/s, size: 31.1 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Colab Notebooks/lost_found_project/final_dataset_slim_distill/train/labels.cache... 2436 images, 27 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2463/2463 3.5Mit/s 0.0s\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 4.0Â±3.7 ms, read: 11.5Â±2.1 MB/s, size: 47.8 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Colab Notebooks/lost_found_project/final_dataset_slim_distill/valid/labels.cache... 755 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 755/755 173.0Kit/s 0.0s\n",
            "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 20, len(boxes) = 813. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
            "Plotting labels to /content/lostfound_slim_exp/yolov8n_student_distilled3/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000417, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 480 train, 480 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/lostfound_slim_exp/yolov8n_student_distilled3\u001b[0m\n",
            "Starting training for 40 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/40      0.73G     0.9291      3.495      1.326         37        480: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 2.1it/s 1:14\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 2.5it/s 9.5s\n",
            "                   all        755        813       0.64      0.232      0.265      0.204\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/40      1.06G     0.9503      2.446      1.345         30        480: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 4.1it/s 37.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.2it/s 7.5s\n",
            "                   all        755        813      0.702      0.367      0.396      0.294\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/40      1.06G     0.9441      2.157      1.301         37        480: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 4.2it/s 36.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.2it/s 7.5s\n",
            "                   all        755        813       0.57      0.456      0.436      0.317\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/40      1.07G     0.8966      1.934      1.269         40        480: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 4.2it/s 36.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.3it/s 7.2s\n",
            "                   all        755        813      0.649       0.48      0.504      0.375\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/40      1.07G     0.8699       1.75      1.239         36        480: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 4.1it/s 37.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.8it/s 6.2s\n",
            "                   all        755        813      0.581      0.483      0.487      0.368\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/40      1.07G     0.8558      1.668      1.232         38        480: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 4.0it/s 38.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.8it/s 6.3s\n",
            "                   all        755        813      0.561      0.574      0.541      0.411\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/40      1.07G     0.8255      1.561       1.21         39        480: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 4.2it/s 37.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.6it/s 6.6s\n",
            "                   all        755        813      0.693      0.551      0.577       0.43\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/40      1.07G     0.8101      1.507      1.193         35        480: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 4.3it/s 36.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.3it/s 7.2s\n",
            "                   all        755        813       0.68      0.554      0.596      0.426\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/40      1.07G     0.7969      1.406      1.184         33        480: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 4.3it/s 35.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.2it/s 7.6s\n",
            "                   all        755        813      0.714      0.547       0.61      0.461\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/40      1.07G     0.7875      1.354      1.176         42        480: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 4.2it/s 36.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.3it/s 7.2s\n",
            "                   all        755        813      0.778      0.546      0.642      0.497\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      11/40      1.07G     0.7814      1.323      1.176         45        480: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 4.1it/s 37.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.6it/s 6.7s\n",
            "                   all        755        813       0.72      0.578      0.638      0.491\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      12/40      1.07G      0.761      1.273      1.162         37        480: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 4.1it/s 37.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 4.0it/s 5.9s\n",
            "                   all        755        813      0.799      0.553      0.649      0.495\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      13/40      1.07G      0.763      1.273      1.165         34        480: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 4.1it/s 37.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.9it/s 6.1s\n",
            "                   all        755        813      0.709      0.632      0.668      0.507\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      14/40      1.07G     0.7537      1.203      1.151         42        480: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 4.1it/s 37.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.4it/s 7.0s\n",
            "                   all        755        813      0.717      0.614      0.647      0.496\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      15/40      1.07G     0.7441      1.156      1.146         54        480: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 4.2it/s 36.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.1it/s 7.7s\n",
            "                   all        755        813      0.758      0.614      0.681      0.529\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      16/40      1.07G     0.7319      1.137      1.143         37        480: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 4.3it/s 36.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.3it/s 7.3s\n",
            "                   all        755        813      0.723      0.603      0.678       0.53\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      17/40      1.07G     0.7287      1.119      1.139         31        480: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 4.2it/s 36.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.4it/s 7.0s\n",
            "                   all        755        813      0.666      0.642      0.682      0.526\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      18/40      1.08G     0.7165      1.089       1.13         32        480: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 4.1it/s 37.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.9it/s 6.1s\n",
            "                   all        755        813      0.746      0.652      0.694      0.534\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      19/40      1.08G     0.6998      1.047      1.122         41        480: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 4.1it/s 37.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 4.2it/s 5.8s\n",
            "                   all        755        813      0.829      0.581      0.707       0.55\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      20/40      1.08G     0.6909      1.037      1.115         36        480: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 4.1it/s 37.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.4it/s 7.1s\n",
            "                   all        755        813      0.778      0.599      0.694      0.539\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      21/40      1.08G     0.7036      1.025      1.124         40        480: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 4.2it/s 36.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.1it/s 7.8s\n",
            "                   all        755        813      0.702      0.641       0.69      0.543\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      22/40      1.08G     0.6801     0.9796      1.115         48        480: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 4.3it/s 36.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.1it/s 7.6s\n",
            "                   all        755        813      0.789      0.626      0.709      0.555\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      23/40      1.08G     0.6711     0.9622      1.103         32        480: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 4.3it/s 35.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.3it/s 7.4s\n",
            "                   all        755        813      0.731      0.665      0.717      0.564\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      24/40      1.08G     0.6748     0.9576      1.107         41        480: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 4.0it/s 38.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.8it/s 6.3s\n",
            "                   all        755        813      0.787      0.644      0.725      0.565\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      25/40      1.08G     0.6685     0.9435      1.105         47        480: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 4.1it/s 37.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.8it/s 6.3s\n",
            "                   all        755        813      0.733      0.676      0.723      0.568\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      26/40      1.08G     0.6633     0.9448      1.102         30        480: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 4.0it/s 38.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.3it/s 7.2s\n",
            "                   all        755        813      0.752       0.63      0.728      0.575\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      27/40      1.08G     0.6409     0.9006      1.087         32        480: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 4.1it/s 37.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.2it/s 7.5s\n",
            "                   all        755        813      0.801      0.665      0.739       0.58\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      28/40       1.1G     0.6342     0.8717      1.086         30        480: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 4.2it/s 36.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.2it/s 7.4s\n",
            "                   all        755        813      0.793      0.613      0.729      0.568\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      29/40       1.1G     0.6416     0.8701      1.087         37        480: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 4.1it/s 37.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.7it/s 6.6s\n",
            "                   all        755        813      0.797      0.634      0.736      0.573\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      30/40       1.1G      0.635       0.85      1.078         33        480: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 4.0it/s 38.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.9it/s 6.1s\n",
            "                   all        755        813      0.763      0.659      0.732       0.58\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      31/40      1.11G     0.4687     0.7096      1.009         15        480: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 4.0it/s 38.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.1it/s 7.7s\n",
            "                   all        755        813      0.697      0.729      0.748        0.6\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      32/40      1.12G     0.4581     0.6452     0.9999         15        480: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 4.4it/s 35.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.2it/s 7.5s\n",
            "                   all        755        813      0.702      0.718      0.753      0.598\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      33/40      1.12G     0.4468     0.6111     0.9882         16        480: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 4.4it/s 35.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.3it/s 7.3s\n",
            "                   all        755        813      0.773        0.7      0.758       0.61\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      34/40      1.12G     0.4357     0.5964     0.9853         15        480: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 4.2it/s 36.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.8it/s 6.3s\n",
            "                   all        755        813      0.736      0.713      0.764      0.617\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      35/40      1.12G     0.4266     0.5759     0.9745         15        480: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 4.2it/s 36.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 4.1it/s 5.9s\n",
            "                   all        755        813      0.753      0.719      0.768      0.621\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      36/40      1.12G     0.4235      0.572     0.9736         15        480: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 4.2it/s 36.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.6it/s 6.6s\n",
            "                   all        755        813       0.77      0.715      0.764      0.621\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      37/40      1.12G     0.4164     0.5531      0.966         17        480: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 4.1it/s 37.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.4it/s 7.2s\n",
            "                   all        755        813      0.762      0.722      0.767      0.623\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      38/40      1.12G     0.4018     0.5348     0.9582         17        480: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 4.2it/s 36.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.1it/s 7.7s\n",
            "                   all        755        813      0.717      0.762      0.778      0.631\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      39/40      1.12G     0.4113     0.5348     0.9665         15        480: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 4.2it/s 36.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 3.5it/s 6.9s\n",
            "                   all        755        813      0.812      0.685      0.773      0.626\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      40/40      1.12G     0.3973     0.5238     0.9594         15        480: 100% â”â”â”â”â”â”â”â”â”â”â”â” 154/154 4.1it/s 37.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 4.0it/s 6.0s\n",
            "                   all        755        813      0.757      0.715      0.771       0.63\n",
            "\n",
            "40 epochs completed in 0.506 hours.\n",
            "Optimizer stripped from /content/lostfound_slim_exp/yolov8n_student_distilled3/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from /content/lostfound_slim_exp/yolov8n_student_distilled3/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating /content/lostfound_slim_exp/yolov8n_student_distilled3/weights/best.pt...\n",
            "Ultralytics 8.3.234 ðŸš€ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,009,548 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 24/24 2.6it/s 9.1s\n",
            "                   all        755        813      0.716      0.762      0.778      0.631\n",
            "                phones        107        108      0.836       0.88      0.926      0.857\n",
            "               jackets        151        151      0.977      0.993      0.995      0.875\n",
            "               glasses         67         68      0.927      0.934      0.967      0.847\n",
            "               airpods         52         74      0.879      0.851      0.862       0.48\n",
            "                 Scarf         30         30      0.849      0.933      0.896      0.762\n",
            "                cables         41         41       0.98          1      0.995      0.762\n",
            "               watches         47         67      0.812      0.772      0.865      0.713\n",
            "bags(other than backpack)         37         37      0.639      0.919      0.836        0.7\n",
            "               wallets         34         34      0.642      0.824      0.846      0.724\n",
            "                  card         35         35      0.945      0.985      0.991      0.878\n",
            "              car-keys         26         29      0.693      0.793      0.807      0.662\n",
            "              Backpack         20         20      0.655        0.9      0.878      0.754\n",
            "               laptops         26         27      0.624      0.741      0.661      0.528\n",
            "                  keys         44         52      0.611      0.769      0.761      0.553\n",
            "             powerbank         11         11      0.736      0.257      0.436      0.401\n",
            "    colby student card          5          5      0.648          1      0.995      0.686\n",
            "               tablets          9          9      0.643      0.602      0.655      0.536\n",
            "              chargers          1          1          0          0          0          0\n",
            "         smart-watches          9          9      0.461       0.29      0.305      0.236\n",
            "                 vests          5          5      0.757        0.8       0.88      0.667\n",
            "Speed: 0.2ms preprocess, 1.8ms inference, 0.0ms loss, 2.7ms postprocess per image\n",
            "Results saved to \u001b[1m/content/lostfound_slim_exp/yolov8n_student_distilled3\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# student validation\n",
        "\n",
        "student_best_path = os.path.join(\n",
        "    \"runs\",\n",
        "    \"detect\",\n",
        "    \"lostfound_slim_exp\",\n",
        "    \"yolov8n_student_distilled\",\n",
        "    \"weights\",\n",
        "    \"best.pt\",\n",
        ")\n",
        "\n",
        "DATASET_SLIM = \"/content/drive/MyDrive/Colab Notebooks/lost_found_project/final_dataset_slim\"\n",
        "DATA_YAML_SLIM = os.path.join(DATASET_SLIM, \"data_slim.yaml\")\n",
        "\n",
        "student_best = YOLO(\"/content/drive/MyDrive/Colab Notebooks/lost_found_project/lostfound_slim_exp/yolov8n_student_distilled/weights/best.pt\")\n",
        "\n",
        "metrics = student_best.val(\n",
        "    data=DATA_YAML_SLIM,  # original slim yaml with original labels\n",
        "    split=\"val\",\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STUDENT VALIDATION METRICS (on GT val)\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"mAP50:     {metrics.box.map50:.3f}\")\n",
        "print(f\"mAP50-95:  {metrics.box.map:.3f}\")\n",
        "print(f\"Precision: {metrics.box.mp:.3f}\")\n",
        "print(f\"Recall:    {metrics.box.mr:.3f}\")\n",
        "print(\"=\" * 60)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLpA_hKggjgM",
        "outputId": "94141f8b-1a4f-404a-caf8-ec594cc221a2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.234 ðŸš€ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,009,548 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.6Â±0.2 ms, read: 0.1Â±0.0 MB/s, size: 47.8 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Colab Notebooks/lost_found_project/final_dataset_slim/valid/labels.cache... 755 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 755/755 973.5Kit/s 0.0s\n",
            "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 20, len(boxes) = 813. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 48/48 1.6it/s 30.1s\n",
            "                   all        755        813      0.711      0.669      0.732       0.57\n",
            "                phones        107        108      0.832      0.824       0.89      0.807\n",
            "               jackets        151        151      0.986      0.993      0.995      0.861\n",
            "               glasses         67         68      0.774      0.856      0.927      0.767\n",
            "               airpods         52         74      0.888       0.86      0.874      0.513\n",
            "                 Scarf         30         30      0.812      0.862      0.908       0.74\n",
            "                cables         41         41      0.994          1      0.995      0.728\n",
            "               watches         47         67      0.775      0.806      0.857      0.693\n",
            "bags(other than backpack)         37         37      0.771      0.784       0.76      0.609\n",
            "               wallets         34         34      0.832      0.729      0.824      0.649\n",
            "                  card         35         35      0.962      0.943      0.986      0.878\n",
            "              car-keys         26         29      0.799      0.655      0.762      0.646\n",
            "              Backpack         20         20      0.746      0.589      0.758      0.621\n",
            "               laptops         26         27      0.585      0.556      0.668      0.509\n",
            "                  keys         44         52      0.692       0.78      0.779      0.578\n",
            "             powerbank         11         11      0.705      0.437      0.535      0.506\n",
            "    colby student card          5          5      0.826          1      0.995      0.459\n",
            "               tablets          9          9      0.423      0.111      0.405      0.266\n",
            "              chargers          1          1          0          0          0          0\n",
            "         smart-watches          9          9      0.465      0.199      0.214      0.175\n",
            "                 vests          5          5      0.354        0.4      0.501      0.396\n",
            "Speed: 1.7ms preprocess, 3.9ms inference, 0.0ms loss, 1.5ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/detect/val3\u001b[0m\n",
            "\n",
            "============================================================\n",
            "STUDENT VALIDATION METRICS (on GT val)\n",
            "============================================================\n",
            "mAP50:     0.732\n",
            "mAP50-95:  0.570\n",
            "Precision: 0.711\n",
            "Recall:    0.669\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import shutil\n",
        "from ultralytics import YOLO\n",
        "import torch\n",
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Load COCO-pretrained YOLO model nano\n",
        "model = YOLO(\"yolov8l.pt\")\n",
        "\n",
        "# Fine-tune on slimmed dataset\n",
        "results = model.train(\n",
        "    data=\"/content/drive/MyDrive/Colab Notebooks/lost_found_project/final_dataset_slim/data_slim.yaml\",\n",
        "    epochs=50,\n",
        "    imgsz=640,\n",
        "    batch=16,\n",
        "    workers=4,\n",
        "    project=\"lostfound_slim_exp\",\n",
        "    name=\"yolov8s_slim_finetune\",\n",
        "    freeze = 10\n",
        ")\n"
      ],
      "metadata": {
        "id": "Kh_gd2otWTg5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}